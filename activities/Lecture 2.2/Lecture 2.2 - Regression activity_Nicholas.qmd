---
title: "Lecture 2.2 - Simple regression activity"
author: "Student"
date: "4/1/2025"
format:
  pdf:
    toc: true
    colorlinks: true
---

```{r}
#| label: setup
#| include: false

# Add libraries here
library(ggplot2)
library(dplyr)
library(kableExtra)
library(tidyverse)
library(ggpubr)
library(broom)
# Load your data here
mcdonalds <- read_csv("mcdonalds.csv")
```
#1. Pick a predictor variable that you think will affect `calories`

#Cholesterol

#Write down your expected relationship between the two variables – specify what you think the correlation will be (high, medium, low, positive, negative)

#Strong, positive, linear correlation between cholesterol (g) and calorie count in Mcdonalds menu items.

#2.	Create a scatterplot of the relationship between the two variables


```{r}
p <- ggplot(mcdonalds, aes(x=Cholesterol, y=Calories)) + geom_point()
r <- cor(mcdonalds$Cholesterol, mcdonalds$Calories, use="complete.obs")
r
sd_y = sd(mcdonalds$Calories)
sd_x = sd(mcdonalds$Cholesterol)
slope = r*sd_y/sd_x
slope
y_int = mean(mcdonalds$Calories) - slope*mean(mcdonalds$Cholesterol)
y_int
p + geom_abline(slope=1.642007, intercept=278.0536)

lmodel <- lm(Calories ~ Cholesterol, data=mcdonalds)
summary(lmodel)
```


```{r}
lmodel.augmented <- augment(lmodel, mcdonalds)

ggplot(lmodel.augmented, aes(x=.resid)) +
  geom_histogram(fill="blue4") +
  labs(x="Residuals", y="Count")
  
ggplot(lmodel.augmented, aes(mcdonalds$Calories, .fitted)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "blue", linetype='dashed') + 
  labs(y = "Residuals", x="Calories")
```


```
```{r}


```


> Remember to replace the parts in `<>` with your own information.

-   How well do you feel your best fit line fits the data?

5.  Next run a regression to estimate the linear relationship between the two variables. We can do this with:

```         
lmodel <- lm(<response.variable> ~ <predictor.variable>, data=<dataset>)
summary(lmodel)
```

Remember to replace `<response.variable>`, `<predictor.variable>` and `<dataset>` with the data and variables you are using.

-   How close to your line was the regression line? If it was different, why do you think that happened?

-   Interpret your results – for every change in your predictor variable, how much does your response variable change?

-   Compared to your expectations, is the predicted change a lot or a little?

## Checking your work

7.  For the final step of part I, add a regression line to your plot. Do this by inserting the following commands into a new code block:

```         
scatterplot <- ggplot(<dataset>, aes(x=<predictor.variable>, y=<response.variable>))+ geom_point()

scatterplot + geom_abline(slope=<regression.slope.estimate>, intercept=<regression.intercept.estimate>)
```

Instead of using your estimate from before, this time enter the slope and intercept generated by the regression in the previous step.

-   How close was your predicted best fit line compared to the regression line R generated? If they were different, why were they different?

-   Would either of your variables benefit from a transformation?

# Simple regression extension

## Calculating residuals

8.  Make a small table using the Quarto (manually make a table in visual mode) built-in table function. Make a table with five columns and four rows. Then pick any three observations in the dataset and, in the table, record the following information:

-   Item name
-   Item's calories
-   Item's observed value for calories
-   The predicted value of calories based on the regression model
-   The size of the residual.

How large were your residuals? Did the size of the residuals indicate to you that the regression line is a good fit or not?

## Residual plotting

9.  Make a residual plot & standard deviation

Fortunately, RStudio can easily make a residual plot. When you run a regression, within the stored regression RStudio stores the values of $\beta$.

Note that you will need to load `broom` library for this code to work. You will be using the `augment()` function that adds the residuals to a dataset for easy display and manipulation.

> Note that in the lecture we viewed the predictor variable on the $x$ axis of the residual plot. Here we are switching to viewing the response variable on the $x$ axis here. If you think about it, the two plots are equivalent, simply rotated. But once we switch to thinking in higher dimensions, the only plot that makes sense is the residual plot with the response variable plotted.

```         
# Save the model
<modelname> <- lm(<predictor.variable> ~ <response.variable>, data=<dataset>)
# Create the residuals database
<modelname>.augmented <- augment(<modelname>, <dataset>)

# Residual histogram plot
ggplot(<modelname>.augmented, aes(x=.resid)) +
  geom_histogram(fill="blue4") +
  labs(x="Residuals", y="Count")
  
# Residual scatterplot  
ggplot(<modelname>.augmented, aes(<response variable>, .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "blue", linetype='dashed') + 
  labs(y = "Residuals", x="<name of response variable>")
```

-   What can you conclude from the residual plots and the standard deviation calculation of the residuals? Does it indicate a good model fit or not?

## Model fit

10. Interpret the `R` squared

What can you conclude about your model based on the `R` squared?

11. Which observations are obvious outliers? How does your line of best fit change if you exclude some of the outliers?

> If you are finished with this analysis, conduct the exercise again for a different predictor variable.

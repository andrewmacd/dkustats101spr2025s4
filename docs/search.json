[
  {
    "objectID": "lectures/lecture.2.3.html#model-fit-measures",
    "href": "lectures/lecture.2.3.html#model-fit-measures",
    "title": "Regression Wisdom",
    "section": "Model fit measures",
    "text": "Model fit measures\n\nRegression results\nMany parts of these results we already know how to interpret. For now, we will focus on model fit measures.\n\n\\(R^2\\)\n\\(s_e\\)\nResiduals 5 number summary\n\n\n\n\n\n\n\n\n\n\n\nResiduals - histogram\n\n\n\n\n\n\n\n\nHow would you interpret this residual histogram?\n\n\n\n\n\n\n\n\n\n\nCorrelation review\n\nRecall that a correlation, \\(r\\), ranges from -1 to 1 and indicates the strength of the association between two variables\n\nIf \\(r\\) is -1 or 1 exactly, there is no variation, the correlation indicate the relationship is a straight line\nNote that \\(r\\) does not indicate the slope\nIf \\(r\\) is 0, that means there is no relationship\n\nWhat is the slope in that case?\n\\(\\hat{y} = b_0 + b_1*x\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nR squared definition\n\n\\(R^2\\) is the square of \\(r\\) in a two variable case, so between 0 and 1\nBut, unlike \\(r\\), \\(R^2\\) is meaningful in multivariate models\nPercent of the total variation in the data explained by the model\nSum of the errors from our model divided by sum of errors from the ‘braindead’ model of \\(\\hat{y}=\\bar{y}\\)\nIf the \\(R^2\\) is small, that means our model doesn’t beat the ‘braindead’ model by very much\n\n\n\n\n\n\n\n\n\n\n\nWhen is \\(R^2\\) “big enough”?\n\n\\(R^2\\) is useful, but only so much so\nThe closer \\(R^2\\) is to 1, the more useful the model\n\nHow close is “close”?\nDepends on the situation\n\\(R^2\\) of 0.5 might be very bad for a model that uses height to predict weight\n\nShould be more closely related\n\n\\(R^2\\) of 0.5 might be very good for a model using test scores to predict future income\n\nResponse variable has a lot of factors that shape it and a lot of noise\n\n\nGood practice: always report \\(R^2\\) and \\(s_e\\) and let readers analyze the results as well"
  },
  {
    "objectID": "lectures/lecture.2.3.html#extrapolating",
    "href": "lectures/lecture.2.3.html#extrapolating",
    "title": "Regression Wisdom",
    "section": "Extrapolating",
    "text": "Extrapolating\n\nThe farther a new value is from the range of \\(x\\), the less trust we should place in the predicted value of \\(y\\)\nVenture into new \\(x\\) territory, called extrapolation\nDubious: questionable assumption that nothing changes about the relationship between x and y changes for extreme values of \\(x\\)\n\n\nPredicting MPG of cars\n1970s data on automobiles\n\n\n\n\n\n\n\n\n\n\nPredicting the Maybach\n\n\n\nMaybach\n\n\n\n\nPredicting the Maybach\n\n\n\nMaybach interior\n\n\n\n\nPredicting the Maybach\n\n\n\nMaybach North Korea visit\n\n\nWill our model do a good job predicting this car’s miles per gallon?\n\n\n\n\n\n\n\n\n\n\nCan we predict this car’s MPG using our model?\nWeight: 6581 pounds\nModel: \\(\\hat{y} = b_0 + b_1*x\\)\n\\(\\hat{y} = 46.3 + -0.00767*6581\\)\n\\(\\hat{y} = -4.17\\) miles per gallon\n\nNonsense prediction\n\n\n\nBe wary of out of sample predictions"
  },
  {
    "objectID": "lectures/lecture.2.3.html#outliers",
    "href": "lectures/lecture.2.3.html#outliers",
    "title": "Regression Wisdom",
    "section": "Outliers",
    "text": "Outliers\n\nHeight and net worth\nFirst, we can create random data for both a variable called height and one called log net worth but in the below example they are defined to be random and have no relation to each other.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding Steve Ballmer\n\n\n\nSteve Ballmer\n\n\n\n\n\n\n\n\n\n\nWhat will happen to the slope and \\(R^2\\)?\n\n\n\n\n\n\n\n\n\n\nSteve Ballmer plot\n\n\n\n\n\n\n\n\n\n\nAdding Warren Buffet\n\n\n\nWarren Buffet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding Aleksandar Vučić\n\n\n\nAleksandar Vučić\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeverage vs. influence\n\nA data point whose \\(x\\) value is far from the mean of the rest of the \\(x\\) values is said to have high leverage.\nLeverage points have the potential to strongly pull on the regression line.\nA point is influential if omitting it from the analysis changes the model enough to make a meaningful difference.\nInfluence is determined by\n\nThe residual\nThe leverage\n\n\n\n\nWarnings\n\nInfluential points can hide in plots of residuals.\nPoints with high leverage pull line close to them, so have small residuals.\nSee points in scatterplot of original data.\nFind regression model with and without the points."
  },
  {
    "objectID": "lectures/lecture.2.3.html#interpreting-a-regression",
    "href": "lectures/lecture.2.3.html#interpreting-a-regression",
    "title": "Regression Wisdom",
    "section": "Interpreting a regression",
    "text": "Interpreting a regression\n\nStep 1: develop some expectations\nHorsepower vs. MPG\n\nMore powerful engines probably are less fuel efficient\nRelationship is likely roughly linear\nThe exact relationship depends on the efficiency of the engine\n\nCould be noisy\n\n\n\n\nStep 2: make a picture\n\n\n\n\n\n\n\n\n\n\nStep 3: check the conditions\n\nQuantitative variable condition\nStraight enough condition\nOutlier condition\nDoes the plot thicken\nConclusion:?\n\n\n\n\n\n\n\n\n\n\n\nStep 4: identify the units\n\nMiles per gallon: amount of miles you can travel on one gallon of gas, a measure of efficiency.\n\nMost gasoline-using cars have MPG between 10-40, higher being better\n\nHorsepower: power of the engine.\n\nTypical values for standard cars are in the 100-200 range, higher meaning more powerful\n\n\n\n\nStep 5: intepret the slope of the regression line\n\n\n\n\n\n\n\n\n\nFor every one unit increase in horsepower, miles per gallon decreases by about -0.15 units\n\nIs that a lot or a little?\n\n\n\n\n\n\n\n\n\n\n\n\nStep 6: determine reasonable values for the predictor variable\n\n\n\n\n\n\n\n\n\n\nStep 7: interpret the intercept\n\n\n\n\n\n\n\n\n\n\nStep 8: solve for reasonable predictor values\nHorsepower = Q1 = 75:\n\\(\\hat{y} = b_0 + b_1*x\\)\n\\(\\hat{y} = 39.94 + -0.158*100\\)\n\\(\\hat{y} = 28.09\\)\nHorsepower = Median = 93.5:\n\\(\\hat{y} = b_0 + b_1*x\\)\n\\(\\hat{y} = 39.94 + -0.158*100\\)\n\\(\\hat{y} = 25.17\\)\nHorsepower = Q3 = 126:\n\\(\\hat{y} = b_0 + b_1*x\\)\n\\(\\hat{y} = 39.94 + -0.158*100\\)\n\\(\\hat{y} = 20.03\\)\n\n\nStep 9: interpret the residuals and identify their units\n\n\n\n\n\n\n\n\n\n\nStep 10: view the distribution of the residuals\n\n\n\n\n\n\n\n\n\n\nStep 11: interpret the residual standard error\n\n\n\n\n\n\n\n\n\n\nStep 12: interpret the R squared\n\n\n\n\n\n\n\n\n\n\nStep 13: think about confounders\n\nWhat are some confounders, or “lurking variables”?\n\nCategorical\nQuantitative"
  },
  {
    "objectID": "lectures/lecture.2.2.html#line-of-best-fit",
    "href": "lectures/lecture.2.2.html#line-of-best-fit",
    "title": "Simple Linear Regression",
    "section": "Line of best fit",
    "text": "Line of best fit\n\n\n\nHDI definition\n\n\nHow to describe the relationship between GDP per capita and HDI score?\nAs we learned:\n\nDirection\nForm\nStrength\nOutlier\n\nWhat do you expect the relationship between GDP per capita and HDI will be?"
  },
  {
    "objectID": "lectures/lecture.2.2.html#scatterplot-of-gdp-per-capita-and-hdi",
    "href": "lectures/lecture.2.2.html#scatterplot-of-gdp-per-capita-and-hdi",
    "title": "Simple Linear Regression",
    "section": "Scatterplot of GDP per capita and HDI",
    "text": "Scatterplot of GDP per capita and HDI\n\n\n\n\n\n\n\n\n\nSmoother\n\n\n\n\n\n\n\n\n\n\nTaking a guess\nWhat do you think the intercept and the slope should be for a line of ‘good’ fit?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast squares line\n\nSlope: 0.000006057761, intercept: 0.589694"
  },
  {
    "objectID": "lectures/lecture.2.2.html#linear-model-1",
    "href": "lectures/lecture.2.2.html#linear-model-1",
    "title": "Simple Linear Regression",
    "section": "Linear model",
    "text": "Linear model\nIt’s better if we come up with a more formal model: \\(\\hat{y}= b_0 + b_1x\\)\n\\(\\hat{y}\\) is our predicted value \\(b_0\\) is the \\(y\\) intercept - the value when \\(x\\) is 0 \\(b_1\\) is the slope\n\nHelps with predictions\n\nFor values not in the sample, we can estimate their HDI score\n\nHelps assess model fit - we can compare different lines more easily\n\nMore specifically we can calculate the residuals\nResiduals are difference between our line and the actually observed value - how much our line ‘missed’ by\n\n\n\nLinear model for our data\n\\(\\hat{y}= 0.589694 + 0.000006057761x\\)"
  },
  {
    "objectID": "lectures/lecture.2.2.html#least-squares-line-1",
    "href": "lectures/lecture.2.2.html#least-squares-line-1",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nBut how to calculate?\nMany different ways\n\nMake a line minimizing the least absolute deviations\nNon-parametric lines\nMake a line minimizing the sum of the squares of the deviations\n\nLeast squares line is most common\n\nAdvantages:\n\nEasy to calculate\nWell understood statistical properties\n\nDisadvantages:\n\nLine will be strongly influenced by outliers"
  },
  {
    "objectID": "lectures/lecture.2.2.html#examining-model-fit",
    "href": "lectures/lecture.2.2.html#examining-model-fit",
    "title": "Simple Linear Regression",
    "section": "Examining model fit",
    "text": "Examining model fit\n\nChecking the residuals\nResidual standard deviation\n\\(R^2\\)\nChecking assumptions\n\n\nChecking the residuals\nAll real datasets have noise so the real formula is:\n\\(y = b_0 + b_1x + e\\)\nResidual = Observed - Predicted\n\n\\(e = y - \\hat{y}\\)\n\nCan easily plot the residuals, put the “size of the miss” on the \\(y\\) axis, and original data on the \\(x\\) axis\n\n\nResiduals - our data"
  },
  {
    "objectID": "lectures/lecture.2.2.html#graphing-the-residuals",
    "href": "lectures/lecture.2.2.html#graphing-the-residuals",
    "title": "Simple Linear Regression",
    "section": "Graphing the residuals",
    "text": "Graphing the residuals\n\n\n\n\n\n\n\n\n\nResiduals vs. observed data"
  },
  {
    "objectID": "lectures/lecture.2.2.html#residual-standard-deviation",
    "href": "lectures/lecture.2.2.html#residual-standard-deviation",
    "title": "Simple Linear Regression",
    "section": "Residual standard deviation",
    "text": "Residual standard deviation\n\nSince the residuals are just another distribution, we can also examine their distribution\n\nWhat to look for: symmetrical, no skew/outliers\nStandard deviation not too large\n\n\n\nResidual standard deviation - our data\n\n\n\n\n\n\n\n\nHow would you interpret this histogram of the residuals?"
  },
  {
    "objectID": "lectures/lecture.2.2.html#r2",
    "href": "lectures/lecture.2.2.html#r2",
    "title": "Simple Linear Regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\\(R^2\\) is just the return of \\(r\\), the correlation coefficient. Remember:\n\n\\(r\\) measures the strength of the association between \\(x\\) and \\(y\\)\n\nThat is, how reliably \\(x\\) varies with \\(y\\)\n\nThe correlation coefficient: 0.74\nOur \\(R^2\\): 0.54\n\n\n\n\n\n\n\n\n\nWhat do you think the \\(R^2\\) will change to when we remove the outlier?\n\n\n\n\n\n\n\n\n\nThe correlation coefficient for a model with the outlier removed:\n\n\n\n\n\n\n\n\n\n\nOur \\(R^2\\) with the outlier removed:"
  },
  {
    "objectID": "lectures/lecture.2.2.html#how-to-interpret-r2",
    "href": "lectures/lecture.2.2.html#how-to-interpret-r2",
    "title": "Simple Linear Regression",
    "section": "How to interpret \\(R^2\\)",
    "text": "How to interpret \\(R^2\\)\n\nIf there are no serious outliers and the relationship is linear, can provide a useful measure of how strongly the predictor variable is related to the response variable\n\nThe two assumptions above are quite strong - you need to always draw a picture to make sure they are true!\nShould not be interpreted as how strongly \\(x\\) causes \\(y\\), we only know about association."
  },
  {
    "objectID": "lectures/lecture.2.2.html#regression-assumptions",
    "href": "lectures/lecture.2.2.html#regression-assumptions",
    "title": "Simple Linear Regression",
    "section": "Regression assumptions",
    "text": "Regression assumptions\n\nQuantitative variable assumption\nStraight enough condition\nOutlier condition\nDoes the plot thicken condition?\n\nHave we met these?"
  },
  {
    "objectID": "lectures/lecture.2.2.html#reexpressions",
    "href": "lectures/lecture.2.2.html#reexpressions",
    "title": "Simple Linear Regression",
    "section": "Reexpressions",
    "text": "Reexpressions\n\nLog reexpressed\n\n\n\n\n\n\n\n\nWhat will happen to the shape of the graph?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog reexpressed - outlier\nAny guess as to the outlier?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutlier\n\n\n\nEquatorial Guinea map\n\n\n\n\n\nPresident’s son\n\n\n\n\n\nPresident’s son’s cars\n\n\n\n\nGraphing the residuals - log\n\n\n\n\n\n\n\n\n\n\nResiduals standard deviation - log\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\nThe correlation coefficient: 0.94\nOur \\(R^2\\): 0.89\n\n\n\nRegression assumptions\nFor the log reexpressed version, have the assumptions been met?\n\nQuantitative variable assumption\nStraight enough condition\nOutlier condition\nDoes the plot thicken condition?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#thoughts-about-comparing-groups",
    "href": "lectures/lecture.1.3.html#thoughts-about-comparing-groups",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Thoughts about comparing groups",
    "text": "Thoughts about comparing groups\n\nFaceted histograms are a reasonable display to show distributions by a categorical variable\n\nHowever these displays become hard to interpret when the number of levels in a category grows large\n\nMuch easier to interpret is side by side box plots\nBox plots capture many important characteristics of a distribution into a summary display\nThink carefully about how you treat outliers\nLet’s view data from the 2023-2024 NBA season"
  },
  {
    "objectID": "lectures/lecture.1.3.html#two-group-comparison",
    "href": "lectures/lecture.1.3.html#two-group-comparison",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Two group comparison",
    "text": "Two group comparison\n\nNBA side-by-side histograms of points scored by W/L\n\n\n\n\n\n\n\n\n\n\nNBA boxplot comparison of points scored by W/L\n\n\n\n\n\n\n\n\n\n\nNBA boxplot comparison of points scored by W/L (better)"
  },
  {
    "objectID": "lectures/lecture.1.3.html#many-group-comparison",
    "href": "lectures/lecture.1.3.html#many-group-comparison",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Many group comparison",
    "text": "Many group comparison\n\nNBA side-by-side histograms of points scored by team\n\n\n\n\n\n\n\n\n\n\nNBA boxplot comparison of points scored by team (better)\n\n\n\n\n\n\n\n\n\n\nYour turn\n\nWork with your neighbor to analyze a different set of statistics\n\nCan be by division or not\nRemember the key features of distributions\n\nShape\nCenter\nSpread\n\n\nInterpret your results"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-outliers---assists",
    "href": "lectures/lecture.1.3.html#checking-outliers---assists",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking outliers - assists",
    "text": "Checking outliers - assists\n\nOutliers - assists\n\n\n\n\n\n\n\n\n\n\nAssists &gt; 40 - true outliers?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-outliers---points",
    "href": "lectures/lecture.1.3.html#checking-outliers---points",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking outliers - points",
    "text": "Checking outliers - points\n\nOutliers - points\n\n\n\n\n\n\n\n\n\n\nPoints by team &gt; 150 - true outliers?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#in-summary",
    "href": "lectures/lecture.1.3.html#in-summary",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "In summary",
    "text": "In summary\n\nThink about which kind of display is appropriate for comparing distributions\nWhen conditioning on a categorical variable, boxplots are usually better\nBut boxplots lose information\nThink carefully about omitting outliers\nOutliers may reveal important information about your dataset!"
  },
  {
    "objectID": "lectures/lecture.1.3.html#titanic-passengers-and-the-normal-distribution",
    "href": "lectures/lecture.1.3.html#titanic-passengers-and-the-normal-distribution",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Titanic passengers and the Normal distribution",
    "text": "Titanic passengers and the Normal distribution\n\n\n\nTitanic\n\n\n\nDataset of passengers on the Titanic\n\n\n\n\n\n\n\n\n\nWhat are your expectations for how age should be distributed?\n\n\n\n\n\n\n\n\n\n\nWe are going to violate our first three rules:\n\nMake a picture\nMake a picture\nMake a picture\n\n\n\n\nWere the passenger ages normally distributed?\nTo answer that question, we need some information about the distribution\nRemember, our main information about distributions is:\n\nShape\nCenter\nSpread\n\n\n\nInformation about age\n\nStandard deviation: 14.4\nMean: 29.9\nNormal model: \\(N(\\mu, \\sigma) = N(29.9,14.4)\\)\n\n\\(\\mu\\) is the theoretical mean\n\\(\\sigma\\) is the theoretical standard deviation\nThese values define the data generating process\nWe only see some values of the data generating process, but if we saw infinite values, the mean would be \\(\\mu\\) and the sd would be \\(\\sigma\\)\nMore on this in the second half of class\n\nHow can we check normality using this information?"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-normality",
    "href": "lectures/lecture.1.3.html#checking-normality",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking normality",
    "text": "Checking normality\n\nThinking about normality\n\nWe can check normality by comparing the quantiles of our data with that of the known quantiles of the normal distribution\n\nWe know approximately 95% of the data lies within two standard deviations\nTherefore, 2.5% data with the lowest values lie outside of -2 standard deviations and 2.5% of data with the highest values lie outside of 2 standard deviations\n\nSimilarly, we know the same information for data within one standard deviation (16%, 68%, 16%)\n\n\n\nData within standard deviations"
  },
  {
    "objectID": "lectures/lecture.1.3.html#checking-against-the-data",
    "href": "lectures/lecture.1.3.html#checking-against-the-data",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Checking against the data",
    "text": "Checking against the data\n\nHistogram of ages from the data\n\n\n\n\n\n\n\n\n\n\nNormality and scaling\n\nNote that normality does not depend on the size of the standard deviation or the size of the mean\nCould easily change the units to be months instead of years\n\nMean would increase a lot\nStandard deviation would increase a lot\nHowever, amount of observations within each standard deviation would stay the same"
  },
  {
    "objectID": "lectures/lecture.1.3.html#final-thoughts-on-normality",
    "href": "lectures/lecture.1.3.html#final-thoughts-on-normality",
    "title": "Lecture 1.3 - Advanced distributions",
    "section": "Final thoughts on normality",
    "text": "Final thoughts on normality\n\nWhen is the normal distribution useful?\n\nWhen we know a data-generating process is normally distributed we don’t even need to sample the population\n\nCan find out exactly how much data is between a certain number of standard deviations\n\nWhen we expect a data-generating process to be normally distributed, can test for deviations from normality\n\nIn the case of Titanic passengers, some parts of the distribution were more bunched up, others more spread out\n\nA lot of our statistical techniques require or work better when the data is ‘roughly’ normal\n\nWill detail these in the coming weeks\n\nWe can transform our data to be closer to normal\n\nNote that transformations won’t work if the data has multiple modes, can only correct skew\n\n\n\n\nWhat transformation would be helpful for age?"
  },
  {
    "objectID": "lectures/lecture.3.2.html#house-price-revisited",
    "href": "lectures/lecture.3.2.html#house-price-revisited",
    "title": "Lecture 3.2 - Confidence Intervals - Means",
    "section": "House price revisited",
    "text": "House price revisited\n\nPrices in King County Houses:\n\n21937 houses\nHighly right skewed\nCan define this as the entire population\nPrices are quantitative\n\n\n\nHouse price graph\n\n\n\n\n\n\n\n\n\n\nDistribution\n\nDistribution:\n\nMin: 75000\nQ1: 685000\nMed: 906000\nQ3: 1355000\nMax: 23000000\nMean: 1152092\nSD: 835505\n\nHighly right skewed\nSD almost as large as the median\nIf a distribution looks like this, what do you think the sampling distribution will look like when n=25? How about when n=200?"
  },
  {
    "objectID": "lectures/lecture.3.2.html#the-central-limit-theorem",
    "href": "lectures/lecture.3.2.html#the-central-limit-theorem",
    "title": "Lecture 3.2 - Confidence Intervals - Means",
    "section": "The central limit theorem",
    "text": "The central limit theorem\n\nThe Central Limit Theorem\n\nThe sampling distribution of any mean becomes nearly Normal as the sample size grows.\n\nRequirements\n\nObservations independent\nRandomly collected sample\n\nThe sampling distribution of the means is close to Normal if either:\n\nLarge sample size\nPopulation close to Normal\n\n\n\nSamples = 100, nn = 200\n\n\n\n\n\n\n\n\n\n\nSamples = 1000, nn = 200\n\n\n\n\n\n\n\n\n\n\nSamples = 100000, nn = 200"
  },
  {
    "objectID": "lectures/lecture.3.2.html#sampling-distribution-shape",
    "href": "lectures/lecture.3.2.html#sampling-distribution-shape",
    "title": "Lecture 3.2 - Confidence Intervals - Means",
    "section": "Sampling distribution shape",
    "text": "Sampling distribution shape\n\nAs number of samples taken goes to infinity, shape of the sampling distribution becomes more clearly normally shaped\nDoesn’t matter the shape of the underlying distribution except for a very few exceptions\nHow about holding samples fixed and changing nn in our sample of a skewed distribution?\n\n\nnn = 10\n\n\n\n\n\n\n\n\n\n\nnn = 25\n\n\n\n\n\n\n\n\n\n\nnn = 50\n\n\n\n\n\n\n\n\n\n\nnn = 100\n\n\n\n\n\n\n\n\n\n\nCentral limit theorem formally\n\nWhen a random sample is drawn from any population with mean μ\\mu and standard deviation σ\\sigma, its sample mean, y‾\\bar{y}, has a sampling distribution with the same mean but whose standard deviation is σn\\frac{\\sigma}{\\sqrt{n}} and we write σ(y‾)=SD(y‾)=σn\\sigma(\\bar{y})=SD(\\bar{y})=\\frac{\\sigma}{\\sqrt{n}}\nNo matter what population the random sample comes from, the shape of the sampling distribution is approximately Normal as long as the sample size is large enough.\nThe larger the sample used, the more closely the Normal approximates the sampling distribution for the mean.\nPractically, nn does not have to be very large for this to work in most cases\n\n\n\nPractical issue with finding the sampling distribution sd\n\nWe almost never know σ\\sigma\nNatural thing is to use sdsamplê\\hat{sd_{sample}}\nWith this, we can estimate the sampling distribution SD with SE:\n\nSE(y‾)=snSE(\\bar{y})=\\frac{s}{\\sqrt{n}}\n\nThis formula works well for large samples, not so much for small\n\nProblem: too much variation in the sample SD from sample to sample\n\nFor smaller nn, need to turn to Gosset and a new family of models depending on sample size"
  },
  {
    "objectID": "lectures/lecture.3.2.html#a-confidence-interval-for-the-mean",
    "href": "lectures/lecture.3.2.html#a-confidence-interval-for-the-mean",
    "title": "Lecture 3.2 - Confidence Intervals - Means",
    "section": "A confidence interval for the mean",
    "text": "A confidence interval for the mean\n\nGosset the brewer\n\n\n\nGuiness\n\n\n\n\nGosset\n\n\n\nGosset\n\n\n\n\nWhat Gosset discovered\n\nAt Guinness, Gosset experimented with beer.\nThe Normal Model was not right, especially for small samples.\nStill bell shaped, but details differed, depending on nn\nCame up with the “Student’s tt Distribution” as the correct model\n\n\n\nA practical sampling distribution model\n\nWhen certain assumptions and conditions are met, the standardized sample mean is:\n\nt=y‾−μSE(y‾)t=\\frac{\\bar{y}-\\mu}{SE(\\bar{y})}\n\nThe t score indicates that the result should be interpreted by a Student’s tt model with n−1n-1 degrees of freedom\nWe can estimate the standard deviation of the sampling distribution by:\n\nSE(y‾)=snSE(\\bar{y}) = \\frac{s}{\\sqrt{n}}\n\n\nDegrees of freedom\n\nFor every sample size nn, there is a different Student’s tt distribution\nDegrees of freedom: df=n−1df=n-1\nSimilar to the n−1n-1 calculation for sample standard deviation\nReason for this is a bit complicated, at this point can just remember to specify tt distribution with n−1n-1\n\n\n\nStudent’s tt\n\n\n\nStudent’s t\n\n\n\n\nOne sample tt interval for the mean\n\nWhen the assumptions are met, the confidence interval for the mean is:\n\ny‾±tn−1×SE(y‾)\\bar{y} \\pm t_{n-1}\\times SE(\\bar{y})\n\nThe critical value, tn−1*t^*_{n-1}, depends on the confidence interval, CC, and the degrees of freedom n−1n-1\n\n\n\nExample: A one sample tt interval for the mean\n\nPrice from one sample in King County\n\n\n\n\n\n\n\n\n\n\n\nAverage house price\n\ny‾±t19*×SE(y‾)\\bar{y}\\pm t^*_{19} \\times SE(\\bar{y})\n1118400±2.09×SE(789011(20))1118400\\pm 2.09 \\times SE(\\frac{789011}{\\sqrt{(20)}})\n1118400±2.09×176428.331118400\\pm 2.09 \\times 176428.33\n[739538−1497262][739538 - 1497262]\n\nWhat is the right way to talk about this confidence interval?\n\n\n\n\n\n\n\n\n\n\nThoughts about zz and tt\n\nThe Student’s t distribution:\n\nIs unimodal.\nIs symmetric about its mean.\nBell-shaped\n\nSamller values of dfdf have longer tails and larger standard deviation than the Normal.\nAs dfdf increase, look more and more like Normal.\nIs needed because we are using s as an estimate for σ\\sigma\nIf you happen to know σ\\sigma, which almost never happens, use the Normal model and not Student’s tt\nAs nn becomes larger, still safe to use the tt distribution because it basically turns into the normal distribution\n\n\n\nAssumptions and conditions\n\nIndependence Assumption\n\nData values should be mutually independent\nExample: weighing yourself every day\n\nRandomization Condition: The data should arise from a random sample or suitably randomized experiment.\n\nData from SRS almost surely independent\nIf doesn’t satisfy Randomization Condition, think about whether values are independent and whether sample is representative of the population.\n\n\n\n\nAssumptions and conditions\n\nNormal Population Assumption\n\nNearly Normal Condition: Distribution is unimodal and symmetric.\nCheck with a histogram.\nn&lt;15n &lt; 15: data should follow a normal model closely. If outliers or strong skewness, don’t use tt-methods\n15&lt;n&lt;4015 &lt; n &lt; 40: tt-methods work well as long as data are unimodal and reasonably symmetric.\nn&gt;40n &gt; 40: tt-methods are safe as long as data are not extremely skewed.\nSimilar to the rule for proportions that must have somewhat even distribution of yeses and noes\n\n\n\n\nExample: Checking Assumptions and Conditions for Student’s tt\n\nPrice of housing in King County\n\nIndependence Assumption: Yes\nNearly Normal Condition: No"
  },
  {
    "objectID": "lectures/lecture.3.2.html#interpreting-confidence-intervals",
    "href": "lectures/lecture.3.2.html#interpreting-confidence-intervals",
    "title": "Lecture 3.2 - Confidence Intervals - Means",
    "section": "Interpreting confidence intervals",
    "text": "Interpreting confidence intervals\n\nWhat not to say\nDon’t say:\n\n“95% of the price of houses in King County is between $739538 and $1497262.”\n\nThe CI is about the mean price, not about the individual houses.\n\n“We are 95% confident that a randomly selected house price will be between $739538 and $1497262.”\n\nAgain, we are concerned here with the mean, not individual houses\n\n\n\n\nWhat not to say continued\nDon’t Say\n\n“The mean price is $1118400 95% of the time.”\n\nThe population mean never changes. Only sample means vary from sample to sample.\n\n“95% of all samples will have a mean price between $739538 and $1497262.”\n\nThis interval does not set the standard for all other intervals. This interval is no more likely to be correct than any other.\n\n\n\n\nWhat you should say\nDo Say\n\n“I am 95% confident that the true mean price is between $739538 and $1497262.”\n\nTechnically: “95% of all random samples will produce intervals that cover the true value.”\n\n\nThe first statement is more personal and less technical."
  },
  {
    "objectID": "lectures/lecture.3.2.html#bootstrapping",
    "href": "lectures/lecture.3.2.html#bootstrapping",
    "title": "Lecture 3.2 - Confidence Intervals - Means",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nPicking our interval up by our bootstraps\n\n\n\n\n\n\n\n\n\n\nKeep in mind\n\nThe confidence interval (unlike the sampling distribution) is centered at y‾\\bar{y} rather than at μ\\mu.\nWe need to know how far to reach out from y‾\\bar{y}, so we need to estimate the population standard deviation. Estimating σ\\sigma means we need to refer to Student’s tt-models.\nUsing Student’s tt-requires the assumption that the underlying data follow a Normal model.\n\nPractically, we need to check that the data distribution of our sample is at least unimodal and reasonably symmetric, with no outliers for n&lt;100n&lt;100.\n\n\n\n\nBootstrapping\nProcess:\n\nWe have a random sample, representative of population.\nMake copies and build a pseudo-population\nSample repeatedly from this population\nFind means\nMake a histogram\nObserve how means are distributed and how much they vary\n\n\n\nBootstrapping\n\n\n\n\n\n\n\n\nHow will this bootstrapping confidence interval compare to the confidence interval calculated by classical means?"
  },
  {
    "objectID": "lectures/lecture.3.2.html#thoughts-about-confidence-intervals",
    "href": "lectures/lecture.3.2.html#thoughts-about-confidence-intervals",
    "title": "Lecture 3.2 - Confidence Intervals - Means",
    "section": "Thoughts about confidence intervals",
    "text": "Thoughts about confidence intervals\n\nConfidence intervals - what’s important\n\nIt’s not their precision.\nOur specific confidence interval is random by nature\nChanges with the sample\nImportant to know how they are constructed\nNeed to check assumptions and conditions\nContains our best guess of the mean\nAnd how precise we think that guess is"
  },
  {
    "objectID": "lectures/lecture.1.2.html#distribution-of-common-quantities",
    "href": "lectures/lecture.1.2.html#distribution-of-common-quantities",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Distribution of common quantities",
    "text": "Distribution of common quantities\nMany phenomena in nature have a relatively easily guessed distribution characteristics\n\nWhat is the distribution of length of rivers in the U.S.?\nWhat is the distribution of width of flower sepals?\nWhat is the distribution of life expectancy across countries in 2007?\n\nFeatures to guess:\n\nShape\nCenter\nSpread"
  },
  {
    "objectID": "lectures/lecture.1.2.html#graphs-of-common-quantities",
    "href": "lectures/lecture.1.2.html#graphs-of-common-quantities",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Graphs of common quantities",
    "text": "Graphs of common quantities\n\nLength of rivers in the U.S.\n\n\n\n\n\n\n\n\n\n\nFlower sepal width\n\n\n\n\n\n\n\n\n\n\nLife expetancy in 2007"
  },
  {
    "objectID": "lectures/lecture.1.2.html#distribution-of-our-data",
    "href": "lectures/lecture.1.2.html#distribution-of-our-data",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Distribution of our data",
    "text": "Distribution of our data\nLet’s now collect some data about our class\n\nInformation about handedness\nInformation about heights"
  },
  {
    "objectID": "lectures/lecture.1.2.html#guessing-the-shape-of-our-data",
    "href": "lectures/lecture.1.2.html#guessing-the-shape-of-our-data",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Guessing the shape of our data",
    "text": "Guessing the shape of our data\nTake a guess at what each question’s distribution characteristics will be:\n\nShape\n\nSkew\nModes\n\nCenter\n\nMean\nMedian\n\nSpread\n\nRange\nIQR\nStandard deviation\n\nAlso think carefully about the difference between the three different calculations of handedness - how do they differ? Discuss with your partner."
  },
  {
    "objectID": "lectures/lecture.1.2.html#height-summary-statistics",
    "href": "lectures/lecture.1.2.html#height-summary-statistics",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Height summary statistics",
    "text": "Height summary statistics\n\nShape\n\nSkew:\nModes: 163\n\nCenter\n\nMean 170.1818182\nMedian 169.5\n\nSpread\n\nRange 152, 188\nIQR 10\nStandard deviation 8.7757044"
  },
  {
    "objectID": "lectures/lecture.1.2.html#height-graph",
    "href": "lectures/lecture.1.2.html#height-graph",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Height graph",
    "text": "Height graph"
  },
  {
    "objectID": "lectures/lecture.1.2.html#handedness-l-r-summary-statistics",
    "href": "lectures/lecture.1.2.html#handedness-l-r-summary-statistics",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Handedness l-r summary statistics",
    "text": "Handedness l-r summary statistics\n\nShape\n\nSkew:\nModes: -12\n\nCenter\n\nMean -12.12\nMedian -12\n\nSpread\n\nRange -16, -8\nIQR 3\nStandard deviation 2.2233608\n\nWhat does this measure?"
  },
  {
    "objectID": "lectures/lecture.1.2.html#handedness-l-r-graph",
    "href": "lectures/lecture.1.2.html#handedness-l-r-graph",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Handedness l-r graph",
    "text": "Handedness l-r graph"
  },
  {
    "objectID": "lectures/lecture.1.2.html#handedness-lr-summary-statistics",
    "href": "lectures/lecture.1.2.html#handedness-lr-summary-statistics",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Handedness l+r summary statistics",
    "text": "Handedness l+r summary statistics\n\nShape\n\nSkew:\nModes: 20\n\nCenter\n\nMean 18.64\nMedian 20\n\nSpread\n\nRange 10, 20\nIQR 2\nStandard deviation 2.3430749\n\nWhat does this measure?"
  },
  {
    "objectID": "lectures/lecture.1.2.html#handedness-lr-graph",
    "href": "lectures/lecture.1.2.html#handedness-lr-graph",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Handedness l+r graph",
    "text": "Handedness l+r graph\n\n\n\n\n\n\n\n\n\nHandedness left - right / left + right graph"
  },
  {
    "objectID": "lectures/lecture.1.2.html#closing-thoughts",
    "href": "lectures/lecture.1.2.html#closing-thoughts",
    "title": "Lecture 1.2 - Characteristics of distributions",
    "section": "Closing thoughts",
    "text": "Closing thoughts\n\nMany distributions can be guessed in advanced based on the data generating process\nYou should have at least a guess as to what the distribution is before starting your exploratory data analysis\nThink carefully about what your variable is actually measuring\nCharacteristics of distributions are summaries of the data, almost always obscure features of the data\nDon’t mislead your readers!!"
  },
  {
    "objectID": "lectures/lecture.3.1.html#the-sampling-distribution-model-for-a-proportion",
    "href": "lectures/lecture.3.1.html#the-sampling-distribution-model-for-a-proportion",
    "title": "Confidence Intervals - Proportions",
    "section": "The sampling distribution model for a proportion",
    "text": "The sampling distribution model for a proportion\n\nSampling model\n\nDraw samples at random, n=100n=100\nSamples vary\nCan’t draw all samples of size 100, astronomical\nDraw a few thousand samples\nDistribution is called the sampling distribution of the proportion.\n\nWhat shape do you think the sampling distribution will have if we have sample size n=100n=100?\n\n\n\n\n\n\n\n\n\n\nGraph of a sampling distribution\n\nRemember, this is not a graph of the actual distribution\n\n\n\n\n\n\n\n\n\n\n\nRandom matters\n\nSampling distribution for a proportion\n\nSymmetric - check\nUnimodal - check\nCentered at pp: 0.853\nStandard deviation: 0.035\nFollows the Normal model - check\n\n\n\n\nThe Normal model for sampling\n\nSamples don’t all have the same proportion.\nNormal model is the right one for sample proportions.\nModeling how sample statistics, proportions or means, vary from sample to sample is powerful.\nAllows us to quantify that variation.\nMake statements about corresponding population parameter.\nMake model for random behavior, then understand and and use that model.\n\n\n\nWhicn Normal model to choose?\n\nReminder: normal model is N(μ,σ2)N(\\mu, \\sigma^2)\nμ\\mu or mean is pp, or the proportion we want to estimate, nn is sample size\nFor proportions, σ(p)=p(1−p)n\\sigma(p) = \\sqrt{\\frac{p(1-p)}{n}}\nThis is the standard deviation of the SAMPLING DISTRIBUTION, that is the distribution of pp across infinite samples\n\n\n\nMean and standard deviation\n\n\n\n\n\n\n\n\n\n\nReminder - Normal model rule\n\nUsing this normal model rule, we can tell how likely it is to have a certain p̂\\hat{p} given the sampling distribution normal model\nRemember the 68–95–99.7 (1 sd, 2 sd, 3 sd), for other distances use technology\nMost common: 95% of samples have sample proportion within two standard deviations of the true population proportion.\nKnowing the sampling distribution tells us how much variation to expect\nCalled the sampling error in some contexts\nNot really an error, just variability\nBetter to call it sampling variability"
  },
  {
    "objectID": "lectures/lecture.3.1.html#when-does-the-normal-model-work",
    "href": "lectures/lecture.3.1.html#when-does-the-normal-model-work",
    "title": "Confidence Intervals - Proportions",
    "section": "When does the normal model work?",
    "text": "When does the normal model work?\n\nIndependence Assumption: check data collected in a way that makes this assumption plausible\nRandomization Condition: subjects randomly assigned treatments, or survey is simple random sample\n10% Condition: sample size less than 10% of the population size\nSuccess Failure Condition: there must be at least 10 expected successes and failures. np̂≥10n\\hat{p}\\geq10 and n̂p≥10\\hat{n}p\\geq10\n\n\nWhen does the normal model fail for the sampling distribution?\n\npp close to 0 or 1\nPeople in this class that can dunk a basketball\nSample size 100\n\nIf true p=0.001p = 0.001, then probably none in sample of 100\n\nIf we simulated samples of size 100 with p=0.001p = 0.001\n\nDistribution skewed right, can’t rely on normal model percentages anymore\n\nnn is fine, but pp is too small\n\nWhat will the shape of the sampling distribution look like if p=0.001p = 0.001?\n\n\n\n\n\n\n\n\n\n\nExample simulation\n\n\n\n\n\n\n\n\n\n\nClass sampling exercise\n\nWe know that about 50% of students at DKU plan to or have selected a major in the natural sciences\n? % of students in our class plan to major in the natural sciences in our class\n\nIs our class unusually small?\n\nCheck conditions\n\nRandomization condition\n10% condition\nSuccess failure condition\n\n\n\n\nFind how far we are from the population mean\n\nPopulation standard deviation formula is:\n\np(1−p)n\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}}\np̂\\hat{p} is the proportion of yeses\nnn is the sample size\n\nWe are calculating using the population sampling SDSD since we know it\n\nIf we don’t know the population sampling SDSD we have to use a different strategy, but not the case here\n\nKnowing the SDSD, we can create a z score for the difference between our class and the population\n\nz score is how many SDSDs our class is from the population mean\n\n(classscore−dkumean)/SD(class score - dkumean) / SD\n\n\n\n\n\nNormal distribution percentages\n\n\n\n\n\n\n\n\n\n\nCalculation for our class\n\np(1−p)n\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}}\np̂\\hat{p} is the proportion of yeses\nnn is the sample size\np̂−pSD(p)\\frac{\\hat{p} - p}{SD(p)}\n68-95-99.7 Rule: Values ?SDSD above the mean occur less than ?% of the time. Our class mean appears to be far/near from the population mean\n\nCalculate the how likely our result would be if our class is a random sample of DKU students."
  },
  {
    "objectID": "lectures/lecture.3.1.html#confidence-intervals-of-proportions",
    "href": "lectures/lecture.3.1.html#confidence-intervals-of-proportions",
    "title": "Confidence Intervals - Proportions",
    "section": "Confidence intervals of proportions",
    "text": "Confidence intervals of proportions\n\nStandard errors for proportions\n\nWhat is the sampling distribution?\nUsually we do not know the population proportion pp.\nTherefore, we cannot find the standard deviation of the sampling distribution p(1−p)n\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}}\nAfter taking a sample, we only know the sample proportion, which we use as an approximation (called the standard error)\n\np̂(1−p̂)n\\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}\n\n\n\n\nExample: bedrooms\n\nDraw a random sample of 100 houses\n\n\n\n\n\n\n\n\n\n\np̂(1−p̂)100\\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{100}}\nThe sampling distribution should be approximately normal\n\n\n\nWhat is a confidence interval?\n\nConfidence interval: a way to express the range of plausible values for the parameter (in this case, percent of homes with three bedrooms)\nWe never know the true value but we want to say something about how wide the range of possible values are\nWhat is a reasonable range?\n\nTraditionally, 95% (about two standard deviations) of the standard error distribution\nMean of our sample ±\\pm range of possible values we could get if we took additional samples\n\n\n\n\nExample: bedrooms\n\nOur mean: 0.87\nOur estimated sampling distribution standard error:\n\np̂(1−p̂)100\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{100}}\n0.87̂(1−0.87̂)100\\sqrt{\\frac{\\hat{0.87}(1-\\hat{0.87})}{100}}\n0.7569100\\sqrt{\\frac{0.7569}{100}}\n0.007569\\sqrt{0.007569}\n0.0870.087\n\nA range of reasonable values if we sampled this again:\n\n2×0.0872\\times0.087\n0.87±0.1740.87\\pm0.174\n\n\nStatement: we are ~95% confident that this interval contains the true proportion of houses with three or more bedrooms in the population\n\n\nCritical values\n\nCritical values are the cutoff we use to determine what is ‘reasonable’\nDerived from the Normal model\nCan use any z-score as a cutoff\nCorresponding multiplier of the SE is called the critical value.\nNormal model for this interval, it is denoted z*z^*.\nTo find, need to use computer, calculator, Normal probability table\n\n\n\n\n\n\n\n\n\n\n\nRecap\n\nMake sure conditions are met, then find level C confidence interval for p̂\\hat{p}, our population mean estimate\nConfidence interval is defined as p̂±z*×SE(p̂)\\hat{p}\\pm z^* \\times SE(\\hat{p})\nSE(p̂)SE(\\hat{p}) estimated by p̂(1−p̂)n\\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}\nz*z^* specifies number of SEs needed for C% of random samples to yield confidence intervals that capture the true parameter\n\n\n\nWhat you cannot say about pp from the sample\n\n“0.87 of all houses in King County have at least three bedrooms.”\n\n\nNo. Observations vary. Another sample would yield a different sample proportion.\n\n\n“It is probably true that 0.87 of all houses in King County have at least three bedrooms.”\n\n\nNo again. In fact, even if we didn’t know the true proportion, we’d know that it’s probably not 0.87.\n\n\n\nWhat you cannot say about pp from the sample\n\n“We don’t know exactly what proportion of houses in King County have at least three bedrooms, but we know that it’s within the interval 0.87±2×0.0870.87\\pm2\\times0.087.”\n\n\n\nNo but getting closer. We don’t know this for sure.\n\n\n“We don’t know exactly what proportion of houses in King County have at least three bedrooms, but the interval from 0.696 to 1.044 probably contains the true proportion.”\n\n\nRight but can be more precise. We should specify how confident we are not just say probably\n\n\n\nWhat you can say about pp from the sample\n\n“We are 95% confident that between 0.696 and 1.044 of houses in King County have at least three bedrooms.”\n\n\nStatements like these are called confidence intervals. They’re the best we can do.\n\n\n\nNaming the confidence interval\n\nThis confidence interval is a one-proportion z-interval.\n\n“One” since there is a single mean being calculated.\n“Proportion” since we are interested in the proportion of the population.\n“z-interval” since the distance of the interval relies on a normal sampling distribution model."
  },
  {
    "objectID": "lectures/lecture.3.1.html#interpreting-confidence-intervals",
    "href": "lectures/lecture.3.1.html#interpreting-confidence-intervals",
    "title": "Confidence Intervals - Proportions",
    "section": "Interpreting confidence intervals",
    "text": "Interpreting confidence intervals\n\nCapturing a proportion\n\nThe confidence interval may or may not contain the true population proportion.\nConsider repeating the study over an over again, each time with the same sample size.\nEach time we would get a different p̂\\hat{p}\nFrom each p̂\\hat{p}, a different confidence interval could be computed.\nAbout 95% of these confidence intervals will capture the true proportion.\n5% will be duds.\n\n\n\nRandom matters - confidence intervals\n\nThere are a huge number of confidence intervals that could be drawn.\nIn theory, all the confidence intervals could be listed.\n\n95% will “work” (capture the true proportion).\n5% will be “duds” (not capture the true proportion).\n\nWhat about our confidence interval (0.696, 1.044)?\n\nIn this case, we can find out the true value\nMost of the time we never know\n\n\n\n\n\n\n\n\n\n\n\n\nRandom matters - confidence intervals\n\n\n\n100 samples CI"
  },
  {
    "objectID": "lectures/lecture.3.1.html#margin-of-error-certainty-vs.-precision",
    "href": "lectures/lecture.3.1.html#margin-of-error-certainty-vs.-precision",
    "title": "Confidence Intervals - Proportions",
    "section": "Margin of error: certainty vs. precision",
    "text": "Margin of error: certainty vs. precision\n\nMargin of error\n\nConfidence interval for a population proportion: p̂±2×SE(p̂)\\hat{p} \\pm 2\\times SE(\\hat{p})\nThe distance, 2×SE(p̂)~2\\times SE(\\hat{p}), from p̂\\hat{p} is called the margin of error\nConfidence intervals can be applied to many statistics, not just means. Regression slopes and other quantities can also have confidence intervals.\n\nIn general, a confidence interval has the form estimate ±\\pm margin of error\n\n\n\n\nCertainty vs. precision\n\nCompeting goals\n\nMore certainty, need to capture pp more often, need to make the interval wider.\nMore precise, need to provider tighter bounds on our estimate for pp, need to make the interval narrower\n\nInstead of a 95% confidence interval, any percent can be used.\n\nIncreasing the confidence (e.g. 99%) increases the margin of error.\n\nNeed to make our range wider to make sure we don’t ‘miss’\n\nDecreasing the confidence (e.g. 90%) decreases the margin of error.\n\nNeed to make our range smaller so as to be more specific about our guess\n\n\n\n\n\nWhat sample size?\n\nCan increase both certainty and precision by increasing sample size\nFor 95%, z*z^* = 1.96\nValues that make ME largest are p̂=0.5\\hat{p}=0.5\nIf we want to ensure, say, a margin of error of &lt;3&lt;3%\n\nME=z×p̂(1−p̂)nME = z\\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n0.03=1.96×(0.5)(0.5)n0.03 = 1.96\\times \\sqrt{\\frac{(0.5)(0.5)}{n}}\n\nSolving for nn, gives n≈1067.1n\\approx1067.1\nWe need to survey at least 1068 to ensure a ME less than 0.03 for the 95% confidence interval.\n\n\n\nThoughts on sample size and ME\n\nObtaining a large sample size can be expensive and/or take a long time.\nFor a pilot study, ME=10ME = 10% can be acceptable.\nFor full studies, ME&lt;5ME &lt; 5% is better.\nPublic opinion polls typically use ME=3ME = 3%, n=1000n = 1000\nIf pp is expected to be very small such as 0.005, then much smaller ME such as 0.1% is required.\n\nCommon in medical studies"
  },
  {
    "objectID": "lectures/lecture.3.4.html#t-vs.-z",
    "href": "lectures/lecture.3.4.html#t-vs.-z",
    "title": "Hypothesis Testing Wisdom",
    "section": "tt vs. zz",
    "text": "tt vs. zz\n\ntt vs. zz test\n\n\n\nKnow σ\\sigma\nVariable is continuous\nVariable is proportion\n\n\n\n\nYes\nz×SDsamplingdist=zσnz\\times SD_{samplingdist}=z\\frac{\\sigma}{\\sqrt{n}}\nz×SDsamplingdist=zpqnz\\times SD_{samplingdist}=z\\sqrt{\\frac{pq}{n}}\n\n\nNo\ntdf×SE=tdfsnt_{df}\\times SE=t_{df}\\frac{s}{\\sqrt{n}}\nDepends on CI vs. hypothesis test\n\n\n\n\n\nWhen to use true population parameter\n\n\n\nKnow σ\\sigma\nHypothesis test\nConfidence interval\n\n\n\n\nYes\npqpq from null\nIf sample is representative, pqpq\n\n\n\nz×SDsamplingdist=zpqnz\\times SD_{samplingdist}=z\\sqrt{\\frac{pq}{n}}\nz×SDsamplingdist=zpqnz\\times SD_{samplingdist}=z\\sqrt{\\frac{pq}{n}}\n\n\nNo\npqpq from null\nUse p̂q̂\\hat{p}\\hat{q}\n\n\n\nz×SE=zpqnz\\times SE=z\\sqrt{\\frac{pq}{n}}\nz×SE=zp̂q̂nz\\times SE=z\\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}"
  },
  {
    "objectID": "lectures/lecture.3.4.html#choosing-the-right-hypothesis-test",
    "href": "lectures/lecture.3.4.html#choosing-the-right-hypothesis-test",
    "title": "Hypothesis Testing Wisdom",
    "section": "Choosing the right hypothesis test",
    "text": "Choosing the right hypothesis test\n\nAlternatives - two sided\n\n\n\nTwo sided test\n\n\n\n\nAlternatives - two sided\n\nOld rate is 20%. H0:p=0.20H_0: p = 0.20\nError could be in either direction HA:p≠0.20H_A: p \\neq 0.20\n\n\n\nAlternatives - one sided\n\n\n\nOne sided test\n\n\n\n\nAlternatives - one sided\n\nInterested only in a decrease in the rate: HA:p&lt;0.20H_A: p &lt; 0.20\nOne-sided alternative\n\n\n\nAlternatives - one sided\n\nOne-sided: pp value is probability only in the direction of the alternative away from the null hypothesis value.\nOne-sided pp value is half the two-sided pp value.\nOne-sided test will reject the null hypothesis more often.\nGreat that it rejects the null hypothesis more often when it is false but not so much when it rejects it more often when it is true!\nTwo-sided rejects less often\nSo use two-sided unless you can justify using one-sided\nAdvantage of two-sided test is results matched to CI."
  },
  {
    "objectID": "lectures/lecture.3.4.html#confidence-interval-test",
    "href": "lectures/lecture.3.4.html#confidence-interval-test",
    "title": "Hypothesis Testing Wisdom",
    "section": "Confidence interval test",
    "text": "Confidence interval test\n\nTwo ways to test a hypothesis\nWay 1: Construct CI, assume null is true, evaluate: is your value a likely mean of null quantity\n\nSelect nullnull\nSelect critical value of tdft_{df}\ntdfSEt_{df}SE or tdfsnt_{df}\\frac{s}{\\sqrt{n}}\nCI: null±tdfSEnull\\pm t_{df}SE\nIs your observed value within the CI?\n\nIf yes, value consistent with null\nIf no, reject null\n\n\n\n\nExample: vineyards\n\n\n\nMy family’s vineyard\n\n\n\n\nExample: vineyards\n\n\n\nFinger Lakes wine district\n\n\n\n\nExample: vineyard size\n\nSelect null: hectaresnull=19.8hectares_{null} = 19.8\nSelect critical value of tdf=0.05t_{df} = 0.05\ntdf×SEt_{df}\\times SE or tdfsn=2.0319.336t_{df}\\frac{s}{\\sqrt{n}}=2.03\\frac{19.3}{\\sqrt{36}}\ny‾=19\\bar{y}=19\nCI: 19±6.519\\pm6.5\nIs the null value within the CI?\nNull: 19.819.8, yes within CI\nCannot reject null"
  },
  {
    "objectID": "lectures/lecture.3.4.html#p-value-test",
    "href": "lectures/lecture.3.4.html#p-value-test",
    "title": "Hypothesis Testing Wisdom",
    "section": "pp value test",
    "text": "pp value test\n\nWay 2: Estimate the specific likelihood of observing your mean if we expect the null hypothesis to be true\n\nSelect null\nSet critical value derived from tdft_{df}\ntscoredifference=observed−nullSEtscore_{difference}=\\frac{observed-null}{SE}\nLook up pp value of that tt score given your dfdf →\\rightarrow that is how unlikely your sample mean is assuming null is true\nCompare to cutoff value\nIf pp smaller than the cutoff value, your sample mean is too unlikely →\\rightarrow reject null\nIf pp larger than cutoff value, your sample mean is within the likely values of the null; do not reject null\n\n\n\nExample: vineyard size\n\nSelect null: hectaresnull=19.8hectares_{null} = 19.8\nSet critical value derived from tdf=0.05t_{df}  = 0.05\ntscoredifference=19−19.83.2=−0.305tscore_{difference}=\\frac{19-19.8}{3.2}=-0.305\nLook up p value of that t score given your df →\\rightarrow that is how unlikely your sample is assuming null is true\n\n\n\n\n\n\n\n\n\n\nCompare to cutoff value: 0.025 &lt; 0.381 &lt; 0.975 →\\rightarrow cannot reject null\n\n\n\nInterpretation\n\nEither provides you with same information; how likely is your sample to be the same as H0H_0\nCI specifies the range of likely values\npp value specifies the specific likelihood\nPractically, pp value often more important\n\nYou can set a cutoff value but reader also needs to know how unlikely your result is\nMaybe they think another cutoff value more important"
  },
  {
    "objectID": "lectures/lecture.3.4.html#interpreting-p-values",
    "href": "lectures/lecture.3.4.html#interpreting-p-values",
    "title": "Hypothesis Testing Wisdom",
    "section": "Interpreting pp values",
    "text": "Interpreting pp values\n\npp value is a conditional probability\n\nThe pp value is the probability of getting results as unusual as observed given that H0H_0 is true.\n\npvalue=p(observedstatistic|H0istrue)p value = p(observed\\ statistic | H_0\\ is\\ true)\npvalue≠p(H0istrue|observedstatistic)p value\\neq p(H_0\\ is\\ true | observed\\ statistic)\n\nThe pp value never gives a probability that H0H_0 is true.\n\npvalue=0.03pvalue = 0.03 does not mean a 3% chance of H0H_0 being correct.\nIt just says that if H0H_0 is correct, then there would be a 3% chance of observing a statistic value more unlike what was observed.\n\n\n\n\nSmall pp values\n\nA smaller pp value provides stronger evidence against H0H_0.\n\nThis does not mean that H0H_0 is less true.\nThe person is not more guilty, you just are more convinced of the guilt.\n\nThere is no hard and fast rule on how small is small enough.\n\nHow believable is H0H_0?\nDo you trust your data?\n\n\n\n\nHow guilty is the suspect?\n\nA bank robbery trial:\n\nBoth robber and defendant are male and same height and weight.\n\npp value getting smaller\n\nRobber wore blue jacket like one found in trash near defendant’s house.\n\npp value still smaller\n\n\nEvidence rolls in, pp value small enough, “beyond reasonable doubt”\n\nHowever, does not make him any guiltier\nJust more confident that made right decision"
  },
  {
    "objectID": "lectures/lecture.3.4.html#alpha-levels-and-critical-values",
    "href": "lectures/lecture.3.4.html#alpha-levels-and-critical-values",
    "title": "Hypothesis Testing Wisdom",
    "section": "Alpha levels and critical values",
    "text": "Alpha levels and critical values\n\nHow to define “rare enough”\n\nNeed to make a decision whether pp value is low enough to reject H0H_0.\nSet a threshold value, called the alpha level (α\\alpha).\npvalue&lt;αpvalue &lt; \\alpha:\n\nReject H0H_0.\nThe results are statistically significant (may not be practically significant!!)\n\nCommon α\\alpha levels are 0.10, 0.05, 0.01, and 0.001.\n0.05 is often used.\nSelect α\\alpha level before collecting data.\n\n\n\nSignificance level\n\nα\\alpha is also called the significance level.\nWhen reject null hypothesis, test is “significant at that level”\nReject the null hypothesis “at the 5% significance level”\nAutomatic nature may be uncomfortable\npp value slightly above alpha level, not allowed to reject the null\npp value slightly below the alpha level leads to rejection\nIf this bothers you, you’re in good company.\nReport the pp value.\n\n\n\nCritical value\n\nFor every null model, an α\\alpha level specifies an area under the curve\nThe cutpoint for that area is called a critical value.\nNormal model: critical value is z*z^*\ntt model: critical value is t*t^*\nWe have used critical values of 1, 2, and 3 when we talked about the 68-95-99.7 Rule.\nLikely to choose α\\alpha levels like 0.1, 0.05, 0.01, or 0.001, with corresponding z*z^* critical values of 1.645, 1.96, 2.576, and 3.29.\nFor a one-sided alternative, divide the α\\alpha level in half."
  },
  {
    "objectID": "lectures/lecture.3.4.html#practical-vs-statistical-significance",
    "href": "lectures/lecture.3.4.html#practical-vs-statistical-significance",
    "title": "Hypothesis Testing Wisdom",
    "section": "Practical vs statistical significance",
    "text": "Practical vs statistical significance\n\nPractical vs statistical significance\n\nStatistical significance\n\npvalue&lt;αpvalue &lt; \\alpha\nIf you don’t know the α\\alpha level, then you don’t know what “significant” means.\nSometimes used to suggest meaningful or important.\nTest with small pp value may be surprising\nBut says nothing about the size of the effect\nThat’s what determines whether the effect makes a difference.\nDon’t be lulled into thinking a statistical significance carries with it any sense of practical importance.\n\n\n\n\nEffect size\n\nPractical significance\n\nReject null hypothesis – care about the actual change or difference in the data\nEffect size – difference between data value and null value\nLarge samples – even a small, unimportant effect size can be statistically significant\nSample not large enough – even a large financially or scientifically important effect may not be statistically significant\nReport CI, pp value – indicates range of plausible values\nCI centered on the observed effect, puts bounds on how big or small the effect size may actually be\n\n\nIf we were making a new drug, what would a reasonable effect size be? What are the factors we should consider? How about when testing the new drug’s safety?"
  },
  {
    "objectID": "lectures/lecture.3.4.html#hypothesis-testing-errors",
    "href": "lectures/lecture.3.4.html#hypothesis-testing-errors",
    "title": "Hypothesis Testing Wisdom",
    "section": "Hypothesis testing errors",
    "text": "Hypothesis testing errors\n\nType I and Type II errors\n\n\n\nType I and Type II errors\n\n\n\n\nType I and Type II errors\n\nType I Error\n\nReject H0H_0 when H0H_0 is true.\n\nType II Error\n\nFail to reject H0H_0 when H0H_0 is false.\n\nMedicine: such as an AIDS test\n\nType I Error →\\rightarrow false positive: patient thinks he has the disease when he doesn’t.\nType II Error →\\rightarrow false negative: patient is told he is disease - free when in fact he has the disease.\n\nJury Decisions\n\nType I: found guilty when the defendant is innocent. Put an innocent person in jail.\nType II: not enough evidence to convict, but was guilty. A murderer goes free.\n\n\n\n\nType I and Type II errors\n\nAs usual, when we conduct a statistical test of significance, we make a statement whether we can accept or reject the null hypothesis at some confidence level (usually 0.05)\nWe may be right or wrong – we never know the true state of the world, only the information obtained in our sample\n\n\n\nType I and Type II errors\n\nSo why not just set significance level at some very high level then? Why not only accept at the 0.001 level?\nSetting a cutoff is actually a tradeoff between Type I vs. Type II statistical errors\nIf a researcher claims support for the research hypothesis with a significant result when, in fact, variations in results are caused by random variables alone, then a Type I error is said to have occurred.\nThrough poor design or faulty sampling, researchers may fail to achieve significance, even though the effect they were attempting to demonstrate actually does exist. In this case it would be said that they had made a Type II error.\n\n\n\nType I and Type II errors\n\n\n\nProbability continuum\n\n\n\n\nProbabilities of Type I and Type II errors\n\np(TypeIError)=αp(Type\\ I\\ Error) = \\alpha\n\nThis represents the probability that if H0H_0 is true then we will reject H0H_0\n\np(TypeIIError)=βp(Type\\ II\\ Error) = \\beta\n\nWe cannot calculate β\\beta. Saying H0H_0 is false does not tell us what the parameter is\n\nDecreasing α\\alpha results in an increase of β\\beta\nReduce β\\beta for all alternatives, by increasing α\\alpha\nThe only way to decrease both is to increase the sample size\n\n\n\nExample: thinking about errors\n\nThe study found patients who took a certain drug to help with diabetes management have an increased risk of heart attack.\nWhat kind of error if their findings were due to chance?\nH0H_0 is true but they rejected H0H_0.\n\nType I error\n\nPatients would be deprived of the diabetes drug’s benefits when there is no increased risk of heart attack.\n\nWhy shouldn’t we try to minimize Type I errors at all costs? What are some scenarios where a Type II error is actually more serious?"
  },
  {
    "objectID": "lectures/lecture.3.4.html#power-of-a-test",
    "href": "lectures/lecture.3.4.html#power-of-a-test",
    "title": "Hypothesis Testing Wisdom",
    "section": "Power of a test",
    "text": "Power of a test\n\nWhat we really want to do is to detect a false null hypothesis.\nWhen H0H_0 is false and we reject it, we have done the right thing.\nA test’s ability to detect a false null hypothesis is called the power of the test.\nPower = 1 – β\\beta\nIf a study fails to reject H0H_0, either:\n\nH0H_0 was true. No error was made.\nH0H_0 is false. Type II error was made.\nJury trial, power: ability of criminal justice system to convict people who are guilty—a good thing!\n\n\n\nExample: errors and power\n\nThe meta study resulted in a larger study of 47 different trials.\nHow could this larger sample size help?\nIf the drug really did increase the chance of heart attack, doctors needed to know.\nMissing this side effect would be a Type II error.\nIncreasing the sample size increases the power of the analysis, increasing the chance of detecting the danger if there is one.\n\n\n\nEffect size\n\nFor same sample size, larger effect size results in higher power\nA small effect size is difficult to detect, high probability of Type II error, lower power\nPower depends on effect size and standard deviation.\nThe effect size depends on “How big a difference would matter?”\n\nIn detecting the “human energy field” would a 53% or a 75% success rate be remarkable?\n\n\n\n\nEffect size\n\nWhen designing an experiment, need to know what effect size would be meaningful\n\nImportant to specify this before conducting your study/experiment\n\nAlso useful to have an expectation about how variable your data are likely to be\nWith those estimates, and a little algebra, you can get an estimate of your required nn\nTo know how variable the data may be, you may need to run a “pilot” study or base calculations on previous similar studies.\nIf you fail to reject H0H_0 hypothesis, then consider whether test had sufficient power.\n\n\n\nExample: sample size, errors, and power\n\nThe diabetes drug manufacturer looked at the study and rebutted that the sample size was too small.\nWhy would this smaller study have been less likely to detect a difference in risks?\n\nSmall studies have more sampling variability.\nSmall studies have less power.\nHard to discern whether the difference is due to chance: Higher p(TypeII)p(Type\\ II), since p(TypeI)p(Type\\ I) is fixed by the FDA.\nLarge studies can reduce the risk of both errors.\nLarger studies are better but very expensive."
  },
  {
    "objectID": "lectures/lecture.3.4.html#ethics",
    "href": "lectures/lecture.3.4.html#ethics",
    "title": "Hypothesis Testing Wisdom",
    "section": "Ethics",
    "text": "Ethics\n\n“…let me put it this way. I think that many or most researchers think of statistical tests as a kind of annoying paperwork, a set of forms they need to fill out in order to get their work published. That’s an impression I’ve had of researchers for a long time: they feel they already know the truth–they conducted the damn experiment already!–so then they find whatever p-values are necessary to satisfy the reviewers.” - Andrew Gelman - Statistics Professor at Columbia\n\n\n\nNeuroskeptic’s 9 layers of science hell"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Monday/Tuesday/Wednesday/Thursday class at 1:15 pm to 2:30 pm in AB1087\nWednesday lab 6:00 pm to 7:15 pm in AB1087\nOffice hours -\n\nMonday 3:00 pm to 5:00 pm in WDR 3114\nWednesday 3:00 pm to 5:00 pm in WDR 3114\n\nHomeworks are all due at 11:59 pm\nAll other announcements and information are posted on the class Teams site"
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Course Introduction",
    "section": "Assessment",
    "text": "Assessment\n\nBeginning of class warmup quiz 10%: At the start of each class session there will be a series of short online questions to start discussion of the class material. I will drop your two lowest quiz grades. I will also curve the grade such that the student with the highest overall quiz grade will receive 100% and all other students will receive a corresponding boost to their grade.\nHomework 60%: At the end of the first two groups of content, a homework will be assigned that will ask you to analyze a dataset and answer questions related to that concept group. These homeworks are due at the noted date at 11:59 pm China time. Each homework will also have a best graph contest, where the person whose classmates vote as having the best graph wins some extra credit points.\n\nHomework 1: 15%\nHomework 2: 20%\nFinal project: 25%\n\nHomework checks 3%: To make sure you are making good progress on your homework, the Sunday before the homework is due (except for the first homework) you will be required to submit your progress on the homework so far. You are required to have tried to have answered all the questions covered by the lectures and textbook up to the specified cutoff. I will not check your answers but rather check to see if you have made a good effort to answer all the questions derived from the material already covered. If you have a reasonable answer for each question checked you will get full points. If you have not made an effort to answer all the questions reviewed you will get a zero.\nUnit 3 exam 20%: Unit 3 comprehension is better checked through an in-class exam. The exam will be open notes but closed book and will take place during the normal lab session that week.\nData Camp labs 5%: A number of labs on the website Data Camp will be assigned to you; these labs are pass/fail and you will receive full credit if you complete each of the labs by the specified due date.\nSyllabus quiz 1%: A short quiz after the first class regarding the course requirements.\nExtra credit maximum 3%: You have several opportunities to earn extra credit. There will be three graph contests, whereby the student who is voted by the other students as having the best graph will receive extra credit points. You can also earn some extra credit by replying to other students who post problems in the Discussion section of the Canvas websiteHomework help` section of Teams."
  },
  {
    "objectID": "index.html#lateness-policy",
    "href": "index.html#lateness-policy",
    "title": "Course Introduction",
    "section": "Lateness policy",
    "text": "Lateness policy\nSince the course moves very quickly, if you are submitting work late that means you are falling behind on other material and it may be difficult for you to recover. Therefore, I have a fairly strict lateness policy.\n\nAll major assignments are due at 11:59:00 pm. Not 11:59:01 or 11:59:31.\nIf it is later than 11:59:00 pm, then the assignment will be assessed a 5% lateness penalty\nIf it is later than 12:29:00 am, then the assignment will be assessed a 10% lateness penalty\nIf it is later than 11:59:00 pm the next day, the assignment will be assessed a 50% lateness penalty\nIf it is later than 2 days from the due date, I will no longer accept the assignment\n\n\nPlease be sure to check that your homework is complete and make sure to submit it a few minutes early. You can submit multiple times on Teams so make sure you have a nearly complete version uploaded even if you want to keep working on it right up to the deadline. I will not be sympathetic to messages that complain of computer problems when you are trying to submit for the first time at 11:58:51 pm."
  },
  {
    "objectID": "index.html#attendance-policy",
    "href": "index.html#attendance-policy",
    "title": "Course Introduction",
    "section": "Attendance Policy",
    "text": "Attendance Policy\nClass will be highly interactive and therefore it is to your advantage to attend class.\nIf you have an unexcused absence from class on Monday or Wednesday, the warmup quiz cannot be made up. If you need to miss class and pre-arrange the absence, there will be a makeup assignment you will need to complete before the start of the next class. If you complete the makeup assignment and submit it in time, the warmup quiz will be excused.\nIf you have an unexcused absence from class on Tuesday, Thursday, or Wednesday lab, your previous warmup quiz will be marked as a zero. If you have an excused absence, makeup work will be assigned."
  },
  {
    "objectID": "index.html#contact-policy",
    "href": "index.html#contact-policy",
    "title": "Course Introduction",
    "section": "Contact Policy",
    "text": "Contact Policy\n\nI usually try to reply to DMs on the same day (though not in the evenings), however responses may be slower on the weekend\nDo not DM me 2 hours before a homework is due and expect an immediate response!\nFor general questions, such as how should one interpret a question on a homework or quiz, or help on debugging, you should ask the Homework help channel in Teams. That way others can benefit from the response or someone else may be able to answer more quickly than I can. If you DM me a question that really belongs in the help channel I will ask you to repost it there.\nMany questions can be answered by carefully checking the class website or reviewing the materials on Teams. If in doubt, though, feel free to ask."
  },
  {
    "objectID": "index.html#chatgpt-and-similar-policy",
    "href": "index.html#chatgpt-and-similar-policy",
    "title": "Course Introduction",
    "section": "ChatGPT (and similar) Policy",
    "text": "ChatGPT (and similar) Policy\nUnless otherwise specified on an assignment, you may use ChatGPT as much as you wish. However, most of the assignments in this class are not very amenable to ChatGPT help. Your main grade will be understanding, annotating, and interpreting your output. Producing the output is the very simple minimal requirement. Also, ChatGPT often makes many coding mistakes. If you rely too much on ChatGPT to code, you will not understand when it makes mistakes and why it makes mistakes."
  },
  {
    "objectID": "index.html#academic-dishonesty-policy",
    "href": "index.html#academic-dishonesty-policy",
    "title": "Course Introduction",
    "section": "Academic Dishonesty Policy",
    "text": "Academic Dishonesty Policy\nDon’t cheat. Don’t be that person. Yes, you. You know exactly what I’m talking about too.\nMore specifically, you are expected to strictly adhere to the Duke Kunshan University Community Standard in all of your work and participation, and violations will be enforced. More details can be found here.\nAll work must be done exclusively by the individual to whom it has been assigned. You should assume that collaboration on assignments, the use of previously-assigned homework, quizzes and answer keys, outside sources or outside aids (both written and electronic) are not allowed unless explicitly noted in the assignment guidelines or in this syllabus. All cases of suspected cheating will be referred for adjudication to the Dean’s Office. Any violation for which a student is found responsible is considered grounds for failure in the course.\nIt may sound cliché to say, but if you cheat and borrow other’s code or answers you are only cheating yourself; you will not learn how to do statistics and doing so will mean you will do worse on the midterm and the final anyway. Cheating is ultimately self-defeating so for both of our benefit, please, don’t do it. If you are having trouble completing the assignment and feel tempted to cheat, please contact me directly instead with the difficulties you are having."
  },
  {
    "objectID": "index.html#disabilities-policy",
    "href": "index.html#disabilities-policy",
    "title": "Course Introduction",
    "section": "Disabilities Policy",
    "text": "Disabilities Policy\nIf you need an accommodation due to a disability, you should not hesitate to request one. The process is that requests should be sent to the Dean of Undergraduate Studies, who will contact me with recommended type of accommodation that is needed. You do not need to disclose your reason for requesting an accommodation with me, and asking through the Dean of Undergraduate Studies helps make things official for both you and me."
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Unit 1 homework check (due Sunday, March 21 at 23:59:00)\n\nUnit 1 homework\n\n\n\nUnit 1 homework (due Sunday, March 28 at 23:59:00)\n\n[Unit 1 homework sample solutions]\n\n\n\nUnit 2 homework check (due Sunday, April 6th at 23:59:00)\n\nUnit 2 homework\n\n\n\nUnit 2 homework (due Sunday, April 13 at 23:59:00)\n\n[Unit 2 homework sample solutions]\n\n\n\nUnit 3 exam (Wednesday, April 23 from 2:45 to 4:00 pm)\n\n[Unit 3 exam solutions]\n\n\n\nFinal exam homework check (due Sunday, May 4 at 23:59:00)\n\nFinal project\n\n\n\nFinal project (due Wednesday, May 7 at 23:59:00)"
  },
  {
    "objectID": "schedulematerials.html",
    "href": "schedulematerials.html",
    "title": "Course Schedule and Class Materials",
    "section": "",
    "text": "Important: class schedule is subject to change, contingent on mitigating circumstances and the progress we make as a class. If there are any changes, I will announce them on Canvas."
  },
  {
    "objectID": "schedulematerials.html#lecture-1.1-class-welcome-monday-tuesday-march-17-18",
    "href": "schedulematerials.html#lecture-1.1-class-welcome-monday-tuesday-march-17-18",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 1.1: Class welcome (Monday & Tuesday, March 17, 18)",
    "text": "Lecture 1.1: Class welcome (Monday & Tuesday, March 17, 18)\nReading to do before class: Chapter 1, 2.1 and 2.2, and 3\nTopics covered:\n\nWhat are data and variables?\nHow to display quantitative and qualitative variables\nContingency tables\nLecture activity: Lecture 1.1 activity\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#lecture-1.2-characteristics-of-distributions-wednesday-thursday-march-19-20",
    "href": "schedulematerials.html#lecture-1.2-characteristics-of-distributions-wednesday-thursday-march-19-20",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 1.2: Characteristics of distributions (Wednesday & Thursday, March 19, 20)",
    "text": "Lecture 1.2: Characteristics of distributions (Wednesday & Thursday, March 19, 20)\nReading to do before class: Chapter 2.3-5 and 4\nTopics covered:\n\nHow to describe the shape, center, and spread of a distribution\nHow to compare distributions\nDealing with problem distributions (outliers, reexpression)\nLecture webpage: Lecture 1.2\nLecture activity: Lecture 1.2 activity\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#lab-1.1-r-and-quarto-familiarization-wednesday-march-19",
    "href": "schedulematerials.html#lab-1.1-r-and-quarto-familiarization-wednesday-march-19",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 1.1: R and Quarto familiarization (Wednesday, March 19)",
    "text": "Lab 1.1: R and Quarto familiarization (Wednesday, March 19)\n\nLab files: Lab 1.1"
  },
  {
    "objectID": "schedulematerials.html#online-lab-1.1-due-march-23-at-115900-pm",
    "href": "schedulematerials.html#online-lab-1.1-due-march-23-at-115900-pm",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 1.1 (due March 23 at 11:59:00 pm)",
    "text": "Online lab 1.1 (due March 23 at 11:59:00 pm)\n\nIntroduction to R\nIntroduction to the Tidyverse"
  },
  {
    "objectID": "schedulematerials.html#unit-1-homework-check-due-march-23-at-235900-pm",
    "href": "schedulematerials.html#unit-1-homework-check-due-march-23-at-235900-pm",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 1 homework check (due March 23 at 23:59:00 pm)",
    "text": "Unit 1 homework check (due March 23 at 23:59:00 pm)\n\nHomework files: Unit 1 homework"
  },
  {
    "objectID": "schedulematerials.html#lecture-1.3-comparing-distributions-and-the-normal-distribution-monday-tuesday-march-24-25",
    "href": "schedulematerials.html#lecture-1.3-comparing-distributions-and-the-normal-distribution-monday-tuesday-march-24-25",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 1.3: Comparing distributions and the Normal distribution (Monday & Tuesday, March 24, 25)",
    "text": "Lecture 1.3: Comparing distributions and the Normal distribution (Monday & Tuesday, March 24, 25)\nReading to do before class: Chapter 5\nTopics covered:\n\nStandard deviation and standardizing values\nNormal models\nNormal percentiles\nLecture webpage: Lecture 1.3\nLecture activity: Lecture 1.3 activity"
  },
  {
    "objectID": "schedulematerials.html#unit-1-homework-due-sunday-march-30-at-235900-pm",
    "href": "schedulematerials.html#unit-1-homework-due-sunday-march-30-at-235900-pm",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 1 homework (due Sunday, March 30 at 23:59:00 pm)",
    "text": "Unit 1 homework (due Sunday, March 30 at 23:59:00 pm)\n\nHomework files: Unit 1 homework\nHomework sample solutions: [Unit 1 homework sample solutions]"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.1-association-and-correlation-wednesday-thursday-march-26-27",
    "href": "schedulematerials.html#lecture-2.1-association-and-correlation-wednesday-thursday-march-26-27",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.1: Association and correlation (Wednesday & Thursday, March 26, 27)",
    "text": "Lecture 2.1: Association and correlation (Wednesday & Thursday, March 26, 27)\nReading to do before class: Chapter 6\nTopics covered:\n\nScatterplots\nCorrelations\nDoes correlation imply causation?\nLecture webpage: Lecture 2.1\nLecture activity: Lecture 2.1 activity"
  },
  {
    "objectID": "schedulematerials.html#lab-2.1-advanced-quarto-editing-wednesday-march-26",
    "href": "schedulematerials.html#lab-2.1-advanced-quarto-editing-wednesday-march-26",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 2.1: Advanced Quarto editing (Wednesday, March 26)",
    "text": "Lab 2.1: Advanced Quarto editing (Wednesday, March 26)\n\nLab files: Lab 2.1\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#online-lab-2.1-due-on-friday-march-28-at-235900",
    "href": "schedulematerials.html#online-lab-2.1-due-on-friday-march-28-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 2.1 (due on Friday, March 28 at 23:59:00)",
    "text": "Online lab 2.1 (due on Friday, March 28 at 23:59:00)\n\nIntroduction to Data Visualization with ggplot2"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.2-simple-linear-regression-monday-tuesday-march-31-april-1",
    "href": "schedulematerials.html#lecture-2.2-simple-linear-regression-monday-tuesday-march-31-april-1",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.2: Simple Linear Regression (Monday & Tuesday, March 31, April 1)",
    "text": "Lecture 2.2: Simple Linear Regression (Monday & Tuesday, March 31, April 1)\nReading to do before class: Chapter 7\nTopics covered:\n\nLine of best fit: least squares\nThe linear model\nWhat are residuals\nRegression assumptions\nLecture webpage: Lecture 2.2\nLecture activity: Lecture 2.2 activity"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.3-regression-wisdom-wednesday-thursday-april-2-3",
    "href": "schedulematerials.html#lecture-2.3-regression-wisdom-wednesday-thursday-april-2-3",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.3: Regression Wisdom (Wednesday & Thursday, April 2, 3)",
    "text": "Lecture 2.3: Regression Wisdom (Wednesday & Thursday, April 2, 3)\nReading to do before class: Chapter 8\nTopics covered:\n\nBeware extrapolation\nOutliers and leverage\nLurking variables\nStraightening scatterplots\nLecture webpage: Lecture 2.3\nLecture activity: Lecture 2.3 activity"
  },
  {
    "objectID": "schedulematerials.html#lab-2.2-working-with-regressions-using-dplyr-wednesday-april-2",
    "href": "schedulematerials.html#lab-2.2-working-with-regressions-using-dplyr-wednesday-april-2",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 2.2: Working with regressions using dplyr (Wednesday, April 2)",
    "text": "Lab 2.2: Working with regressions using dplyr (Wednesday, April 2)\n\nLab files: Lab 2.2\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#online-lab-2.2-due-on-friday-april-4-at-235900",
    "href": "schedulematerials.html#online-lab-2.2-due-on-friday-april-4-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 2.2 (due on Friday, April 4 at 23:59:00)",
    "text": "Online lab 2.2 (due on Friday, April 4 at 23:59:00)\n\nIntermediate Data Visualization with ggplot2"
  },
  {
    "objectID": "schedulematerials.html#unit-2-homework---progress-check-due-sunday-april-6-at-235900",
    "href": "schedulematerials.html#unit-2-homework---progress-check-due-sunday-april-6-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 2 homework - progress check (due Sunday, April 6 at 23:59:00)",
    "text": "Unit 2 homework - progress check (due Sunday, April 6 at 23:59:00)\n\nHomework files: Unit 2 homework instructions"
  },
  {
    "objectID": "schedulematerials.html#lecture-2.4-multiple-regression-monday-tuesday-april-7-8",
    "href": "schedulematerials.html#lecture-2.4-multiple-regression-monday-tuesday-april-7-8",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 2.4: Multiple Regression (Monday & Tuesday, April 7, 8)",
    "text": "Lecture 2.4: Multiple Regression (Monday & Tuesday, April 7, 8)\nReading to do before class: Chapter 9\nTopics covered:\n\nWhat is multiple regression?\nInterpreting multiple regression coefficients\nPartial regression plots\nIndicator variables\nLecture webpage: Lecture 2.4\nLecture activity: Lecture 2.4 activity"
  },
  {
    "objectID": "schedulematerials.html#unit-2-homework-due-sunday-april-13-at-235900",
    "href": "schedulematerials.html#unit-2-homework-due-sunday-april-13-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Unit 2 homework (due Sunday, April 13 at 23:59:00)",
    "text": "Unit 2 homework (due Sunday, April 13 at 23:59:00)\n\nHomework files: Unit 2 homework instructions"
  },
  {
    "objectID": "schedulematerials.html#lab-2.3-interpreting-coefficients-wednesday-april-9",
    "href": "schedulematerials.html#lab-2.3-interpreting-coefficients-wednesday-april-9",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 2.3: Interpreting coefficients (Wednesday, April 9)",
    "text": "Lab 2.3: Interpreting coefficients (Wednesday, April 9)\n\nLab files: Lab 2.3\n\nMake sure to extract (unzip) the lab files before attempting to modify them!"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.1-confidence-intervals---proportions-wednesday-thursday-april-9-10",
    "href": "schedulematerials.html#lecture-3.1-confidence-intervals---proportions-wednesday-thursday-april-9-10",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.1: Confidence intervals - proportions (Wednesday & Thursday, April 9, 10)",
    "text": "Lecture 3.1: Confidence intervals - proportions (Wednesday & Thursday, April 9, 10)\nReading to do before class: Chapter 13\nTopics covered:\n\nWhat is a sampling distribution?\nWhen does the normal model apply?\nConstructing a confidence interval\nInterpreting a confidence interval\nLecture webpage: Lecture 3.1\nLecture activity: Lecture 3.1 activity"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.2-confidence-intervals---means-monday-tuesday-april-14-15",
    "href": "schedulematerials.html#lecture-3.2-confidence-intervals---means-monday-tuesday-april-14-15",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.2: Confidence intervals - means (Monday & Tuesday, April 14, 15)",
    "text": "Lecture 3.2: Confidence intervals - means (Monday & Tuesday, April 14, 15)\nReading to do before class: Chapter 14\nTopics covered:\n\nThe Central Limit Theorem\nConfidence interval for means\nInterpreting a confidence interval\nFinal thoughts on confidence intervals\nLecture webpage: Lecture 3.2\nLecture activity: Lecture 3.2 activity"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.3-hypothesis-testing-wednesday-thursday-april-16-17",
    "href": "schedulematerials.html#lecture-3.3-hypothesis-testing-wednesday-thursday-april-16-17",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.3: Hypothesis testing (Wednesday & Thursday, April 16, 17)",
    "text": "Lecture 3.3: Hypothesis testing (Wednesday & Thursday, April 16, 17)\nReading to do before class: Chapter 15\nTopics covered:\n\nWhat are hypotheses?\n\\(p\\) values\n\\(p\\) values and decisions – how to make a decision\nLecture webpage: Lecture 3.3\nLecture activity: Lecture 3.3 activity"
  },
  {
    "objectID": "schedulematerials.html#lab-3.1-bootstrapping-wednesday-april-16",
    "href": "schedulematerials.html#lab-3.1-bootstrapping-wednesday-april-16",
    "title": "Course Schedule and Class Materials",
    "section": "Lab 3.1: Bootstrapping (Wednesday, April 16)",
    "text": "Lab 3.1: Bootstrapping (Wednesday, April 16)\n\nLab files: Lab 3.1\n\nMake sure to extract (unzip) the lab files before attempting to modify them!\n\n\n** Note: no class on Monday, April 21st"
  },
  {
    "objectID": "schedulematerials.html#lecture-3.4-hypothesis-testing-wisdom-tuesday-april-22",
    "href": "schedulematerials.html#lecture-3.4-hypothesis-testing-wisdom-tuesday-april-22",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 3.4: Hypothesis testing wisdom (Tuesday, April 22)",
    "text": "Lecture 3.4: Hypothesis testing wisdom (Tuesday, April 22)\nReading to do before class: Chapter 16\nTopics covered:\n\nInterpreting p-values\nAlpha and critical values\nPractical vs. statistical significance\nType I and II errors\nPower of a test\nEthical issues\nLecture webpage: Lecture 3.4"
  },
  {
    "objectID": "schedulematerials.html#lecture-4.1-comparing-groups-wednesday-friday-april-23-25",
    "href": "schedulematerials.html#lecture-4.1-comparing-groups-wednesday-friday-april-23-25",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 4.1: Comparing groups (Wednesday & Friday, April 23, 25)",
    "text": "Lecture 4.1: Comparing groups (Wednesday & Friday, April 23, 25)\nReading to do before class: Chapter 17\nTopics covered:\n\nConfidence intervals for comparing two samples\nAssumptions and conditions for two-sample hypothesis tests\nTwo-sample \\(z\\) test\nTwo-sample \\(t\\) test\nLecture webpage: Lecture 4.1\nLecture activity: Lecture 4.1 activity"
  },
  {
    "objectID": "schedulematerials.html#in-class-unit-3-exam-thursday-thursday-april-24",
    "href": "schedulematerials.html#in-class-unit-3-exam-thursday-thursday-april-24",
    "title": "Course Schedule and Class Materials",
    "section": "In-class Unit 3 exam: Thursday, Thursday, April 24",
    "text": "In-class Unit 3 exam: Thursday, Thursday, April 24"
  },
  {
    "objectID": "schedulematerials.html#lecture-4.2-returning-to-regression-monday-tuesday-april-28-29",
    "href": "schedulematerials.html#lecture-4.2-returning-to-regression-monday-tuesday-april-28-29",
    "title": "Course Schedule and Class Materials",
    "section": "Lecture 4.2: Returning to regression (Monday & Tuesday, April 28, 29)",
    "text": "Lecture 4.2: Returning to regression (Monday & Tuesday, April 28, 29)\nReading to do before class: Chapter 20\nTopics covered:\n\nRegression inference and intuition\nThe regression table\nConfidence and prediction intervals\nLecture webpage: Lecture 4.2\nLecture activity: Lecture 4.2 activity"
  },
  {
    "objectID": "schedulematerials.html#final-class-4.3-interpretation-activity-model-building-practice-wednesday-april-30",
    "href": "schedulematerials.html#final-class-4.3-interpretation-activity-model-building-practice-wednesday-april-30",
    "title": "Course Schedule and Class Materials",
    "section": "Final class 4.3: Interpretation activity & model building practice (Wednesday, April 30)",
    "text": "Final class 4.3: Interpretation activity & model building practice (Wednesday, April 30)\n\nHow to read academic statistical results\nLocating the model\nInterpreting the test\nDetermining possible weaknesses of the model\nFinal class reading: Final class 4.3 reading\nFinal class activity: [Final class 4.3 activity]"
  },
  {
    "objectID": "schedulematerials.html#online-lab-4.1-due-on-friday-may-2-at-115900",
    "href": "schedulematerials.html#online-lab-4.1-due-on-friday-may-2-at-115900",
    "title": "Course Schedule and Class Materials",
    "section": "Online lab 4.1 (due on Friday, May 2 at 11:59:00)",
    "text": "Online lab 4.1 (due on Friday, May 2 at 11:59:00)\n\nYour choice of any DataCamp course (as long as it relates to statistics)\n\nSend the completion certificate from the end of the course"
  },
  {
    "objectID": "schedulematerials.html#final-project---progress-check-due-sunday-may-4-at-235900",
    "href": "schedulematerials.html#final-project---progress-check-due-sunday-may-4-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Final project - progress check (due Sunday, May 4 at 23:59:00)",
    "text": "Final project - progress check (due Sunday, May 4 at 23:59:00)\n\nHomework files: Final project instructions"
  },
  {
    "objectID": "schedulematerials.html#final-project-due-wednesday-may-7-at-235900",
    "href": "schedulematerials.html#final-project-due-wednesday-may-7-at-235900",
    "title": "Course Schedule and Class Materials",
    "section": "Final project (due Wednesday, May 7 at 23:59:00)",
    "text": "Final project (due Wednesday, May 7 at 23:59:00)\n\nHomework files: Final project instructions"
  },
  {
    "objectID": "lectures/lecture.3.3.html#hypotheses",
    "href": "lectures/lecture.3.3.html#hypotheses",
    "title": "Lecture 3.3 - Hypothesis Testing",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nKing county\n\n\n\nOne sided tests\n\n\n\n\nPercent of houses with a view\n\n11% of houses in King County have a view\nPut ourselves in the shoes of a a researcher – we only have a budget to drive around and sample 100 houses in a particular neighborhood\nDoes our sample represent the the idea that the neighborhood we chose to sample has a higher or lower percentage of houses with a view as compared to the overall county?\n\n\n\nHypotheses\n\nThe starting hypothesis to be tested is call the null hypothesis – null because it assumes that nothing has changed.\n\nWe denote it H0H_0\nCalled “H naught”\n\nH0H_0: parameter = hypothesized value\nH0H_0: has.a.view = 0.11\n\n\nThe alternative hypothesis is not a single value, it contains all other values\n\nHAH_A: parameter ≠\\neq hypothesized value\nHAH_A: has.a.view ≠\\neq 0.11\n\n\n\n\nHow small to convince us?\n\nIf the percent of houses with a view in our sample is 16%, we would be skeptical that there was any difference\n\nNot so unlikely to be different only by random chance\n\nIf the percent of houses with a view is 31%, it would clearly indicate a change from 11%\n\nExtremely unlikely that this could happen just by random chance\n\nCan turn to confidence interval\n\nStandard deviation of the sampling distribution\nSD(p)=pqn=0.11×0.89100=0.031SD(p) = \\sqrt{\\frac{pq}{n}} = \\sqrt{\\frac{0.11\\times0.89}{100}} = 0.031\n\n\n\n\nIs the rate of houses with a view different?\n\nIdea: simulation\n\n\n\n\n\n\n\n\n\n\n\nIs the rate of houses with a view different?\n\nIn 95% of samples of this size, percent of houses with a view is within 5% to 17% just by chance\nIn any given sample, should see between 5% and 17% of houses with a view\nIf we surveyed a neighborhood and actually saw 14%\nSurprising?\nShould we conclude that the percent of houses with a view in this neighborhood is different overall compared to King County?\n\n\n\nCLT means we can use the Normal model\n\nCentral Limit Theorem: use Normal model to find probability instead of using a simulation\nView rate of 0.14, 0.03 from null hypothesis, 0.11\nUse the SD = 0.031 to find the area in the tails that lie more than 0.973 zz scores away from the null hypothesis\nIndicates how rare observed rate is\nProbability is 0.165 in each tail, or about 0.33 total that we would get an observation this far or father from the mean by chance\n\n\n\nConclusion\n\nA difference in view percentages of houses of 0.03 or larger would happen about 33% of the time just by chance.\nDoes/Doesn’t seem very unusual.\n\nBoth?\n\nSo observed proportion of 0.14 does/doesn’t provide evidence that this sample has a different percentage of houses with a view\n\nBorderline\n\n\n\n\nA trial as a hypothesis test\n\nBegin with the presumption of innocence (H0H_0)\nCollect evidence\n\nBank money in house\nStill wearing mask\nGetaway car found in his name\n\nEvidence beyond a reasonable doubt?\n\nIs 5% small enough chance?\nHow about 1%?\n6.7%?\n\n\n\n\nA trial as a hypothesis test - decision\n\nBeyond a reasonable doubt - ambiguous\nJury does not use probability to decide.\nNull hypothesis - quantify exactly how surprising the evidence would be if the null hypothesis were true\nHow unlikely is unlikely?\n1 out of 20, 5%, or 0.05\n1 out of 100, 1%, or 0.01\nYou must judge for yourself in each situation whether the probability of observing your data is small enough to constitute “reasonable doubt.”"
  },
  {
    "objectID": "lectures/lecture.3.3.html#p-values",
    "href": "lectures/lecture.3.3.html#p-values",
    "title": "Lecture 3.3 - Hypothesis Testing",
    "section": "pp values",
    "text": "pp values\n\nThe pp value and surprise\n\nThe pp value is the probability of seeing data like these (or even more unlikely data) given the null hypothesis is true.\nTells us how surprised we would be to get these data given H0H_0 is true.\n\npp value is very low: Either H0H_0 is not true or something remarkable occurred. Reject H0H_0.\npp value is high: Not a surprise. Data consistent with the model. Do not reject H0H_0.\n\n\n\n\nGuilty or not enough evidence?\n\nDefendant is either:\n\nGuilty: pp value too small. The evidence is clear.\nNot Guilty: pp value not small enough. The evidence is not sufficient. Not the same as innocent.\n\nMay be innocent or may be guilty, but not enough evidence found.\n\n\nTwo Choices\n\nFail to reject H0H_0 if pp value large.\n\nNever accept H0H_0.\n\nReject H0H_0 if pp value is small. Accept HAH_A.\n\n\n\n\nWhen the pp value is not small\n\nIt is wrong to say:\n\nAccept H0H_0.\nWe have proven H0H_0.\n\nIt is correct to say:\n\nFail to reject H0H_0\nThere is insufficient evidence to reject H0H_0.\nH0H_0 may or may not be true.\n\nExample: H0H_0: All swans are white.\n\nIf we sample 100 swans that are all white, there could still be a nonwhite swan."
  },
  {
    "objectID": "lectures/lecture.3.3.html#the-reasoning-of-a-hypothesis-test",
    "href": "lectures/lecture.3.3.html#the-reasoning-of-a-hypothesis-test",
    "title": "Lecture 3.3 - Hypothesis Testing",
    "section": "The reasoning of a hypothesis test",
    "text": "The reasoning of a hypothesis test\n\nStep 1: state the hypothesis\n\nH0H_0:\n\nH0H_0 usually states that there’s nothing different.\nH0:H_0: parameter = hypothesized value\nNote the parameter describes the population not the sample.\nH0H_0 is called the null hypothesis.\n\nHAH_A:\n\nHAH_A is a statement that something has changed, gotten bigger or smaller\nHAH_A is called the alternative hypothesis.\n\n\n\n\nHypotheses about breakfast\n\n\n\nBreakfast study\n\n\nWhat would our hypotheses be if we did a study of DKU students and compared it to the percentage found in this paper (treat it as the population mean)?\n\n\n\n\n\n\n\n\n\n\nHypotheses about breakfast\n\nOur data claims that 65.6% of students don’t eat breakfast.\nIn a survey of 26 students at DKU, 11 ate breakfast.\nIs there evidence that breakfast rate is below 65.6%?\n\nH0:p=0.656H_0: p = 0.656\nHA:p≠0.656H_A: p \\neq 0.656\n\n\n\n\nStep 2: the model\n\nDecide on the model to test the null hypothesis and parameter.\nCheck conditions, e.g. independence, sample size.\nIf the conditions are not met, either quit or redesign the study.\nNormal models use zz scores. Other models may not use zz scores.\nName the model, e.g. 1-proportion zz test.\n\n\n\nStep 2a: 1-proportion zz test\n\nConditions: same as a 1-Proportion zz interval\nNull hypothesis: H0:p=p0H_0: p = p_0\nTest statistics\n\nz=p̂−p0SD(p̂)z = \\frac{\\hat{p}-p_0}{SD(\\hat{p})}\nSE(p̂)=p0q0nSE(\\hat{p}) = \\sqrt{\\frac{p_0q_0}{n}}\n\n\nWhat conditions do we need to check to do a 1-proportion zz test?\n\n\n\n\n\n\n\n\n\n\nStep 2b: checking conditions - eating breakfast\n\nRandomization Condition: The 26 students were a random selection of DKU students\n10% Condition: 26 is fewer than 10% of the total number of all students who are of interest.\nSuccess/Failure Condition:\nnp0=(26)(0.656)=17&gt;10np_0 = (26)(0.656) = 17 &gt; 10?\nnq0=(26)(0.344)=9&gt;10nq_0 = (26)(0.344) = 9 &gt; 10?\nThe conditions are satisfied. We can use the Normal model and perform a 1-Proportion z-Test.\n\n\n\nStep 3: mechanics\n\nClaim: 65.6% students eat breakfast.\n\n11 students in our class ate breakfast.\n\nFind pp value\n\nSE(p̂)=0.656⋅0.34426SE(\\hat{p})=\\sqrt{\\frac{0.656\\cdot0.344}{26}}\nzz score of difference = 0.423−0.6560.093=−2.5\\frac{0.423 - 0.656}{0.093} = -2.5\n\nProbability of seeing that large a difference: pnorm(−2.5)→pnorm(-2.5)\\rightarrow 1.2% of the time we will see a difference from the mean this large or larger\n\n98.8% of the time we will see a difference this small or smaller.\nNote: pp values usually reported as “this large or larger”\n\n\nWhat can we conclude from these results?\n\n\n\n\n\n\n\n\n\n\nStep 4: conclusion\n\nIs our sample different than the overall mean of students in the US? pp value = 0.012\nWhat can be concluded? What does the pp value mean?\n\npp value = 0.012 →\\rightarrow Fail to Reject/Reject H0H_0\nThe survey data does not provide strong evidence that the rate that DKU students eat breakfast is different than US average.\nThis should not be the end of the conversation.\nThe next step would be to see if the breakfast eating rate is lower for all classes/among freshman/sophomores/etc."
  },
  {
    "objectID": "lectures/lecture.3.3.html#a-hypothesis-test-for-the-mean",
    "href": "lectures/lecture.3.3.html#a-hypothesis-test-for-the-mean",
    "title": "Lecture 3.3 - Hypothesis Testing",
    "section": "A hypothesis test for the mean",
    "text": "A hypothesis test for the mean\n\nOne sample tt test for the mean\n\nAssumptions are the same.\nH0:μ=μ0H_0: \\mu=\\mu_0\ntn−1=y‾−μ0SE(y‾)t_{n-1}=\\frac{\\bar{y}-\\mu_0}{SE(\\bar{y})}\nStandard Error of y‾:SE(y‾)=σn\\bar{y}: SE(\\bar{y})=\\frac{\\sigma}{\\sqrt{n}}\nWhen the conditions are met and H0H_0 is true, the statistic follows the Student’s tt Model with n–1n – 1 df.\nUse this model to find the pp value."
  },
  {
    "objectID": "lectures/lecture.3.3.html#interval-and-tests",
    "href": "lectures/lecture.3.3.html#interval-and-tests",
    "title": "Lecture 3.3 - Hypothesis Testing",
    "section": "Interval and tests",
    "text": "Interval and tests\n\nIntervals and tests\n\nConfidence Intervals\n\nStart with data and find plausible values for the parameter. CI is data-centric.\nAlways 2-sided\n\nHypothesis Tests\n\nUsing CI, is proposed parameter value consistent with interval? Test is model-centric.\n2-sided test: Within the confidence interval means fail to reject H0H_0.\n\npp value = 1 – C is the cutoff.\n\n\n\n\nThe special case with proportions\n\nConfidence intervals\n\nUse p̂\\hat{p} to calculate SE(p̂)=p̂q̂nSE(\\hat{p})=\\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}\n\nHypothesis Tests\n\nUse p̂\\hat{p} to calculate SD(p̂)=pqnSD(\\hat{p})=\\sqrt{\\frac{pq}{n}}\n\n\nIf SE(p̂)SE(\\hat{p}) and SD(p̂)SD(\\hat{p}) are far from each other, the relationship between the confidence interval and the hypothesis test breaks down."
  },
  {
    "objectID": "lectures/lecture.3.3.html#p-values-and-decisions-what-to-tell-about-a-hypothesis-test",
    "href": "lectures/lecture.3.3.html#p-values-and-decisions-what-to-tell-about-a-hypothesis-test",
    "title": "Lecture 3.3 - Hypothesis Testing",
    "section": "pp values and decisions: what to tell about a hypothesis test",
    "text": "pp values and decisions: what to tell about a hypothesis test\n\nHow small a pp value is small enough?\n\nHow small is small enough is context specific.\nTest to see if a renowned musicologist can distinguish between Mozart and Hayden.\n\npp value of 0.1 may be good enough. Just reaffirming known talent.\n\nA friend claims psychic abilities and can predict heads or tails.\n\nVery small pp value such as 0.01 needed. Breaking scientific theory.\n\n\n\n\nAcceptable pp value depends on result’s importance\n\nProportion of students with full time jobs has increased.\n\nNot that important. pp value = 0.05 will work.\n\nTesting the strength of a bridge based on a sample of concrete quality\n\nLife and death decision. Need a very small pp value\n\nWhether rejecting or failing to reject, always cite the pp value.\nAn accompanying confidence interval helps also.\n\n\n\nRecommendations from the American Statistical Association\n\npp values can indicate how incompatible the data are with a specified statistical model.\npp values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a pp value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA pp value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a pp value does not provide a good measure of evidence regarding a model or hypothesis."
  },
  {
    "objectID": "lectures/lecture.4.1.html#what-type-of-inference-is-a-t-test",
    "href": "lectures/lecture.4.1.html#what-type-of-inference-is-a-t-test",
    "title": "Lecture 4.1 - Comparing Groups",
    "section": "What type of inference is a tt test?",
    "text": "What type of inference is a tt test?\n\nA review - descriptive vs. inference\n\n\n\n\nTable 1: Inference types - general\n\n\n\n\n\n\nType of analysis\nDescriptive\nInferential\n\n\n\n\nUnivariate\nHistogram, bar chart\nConfidence interval\n\n\nUnivariate compared to theoretical expectation\nQQ plot\nOne proportion z test, one proportion t test\n\n\nComparing two variables\nScatterplot, two variable regression\nTwo proportion z test, two proportion t test\n\n\nComparing many variables\nMultiple variable regression\nMultiple variable regression\n\n\n\n\n\n\n\n\n\n\n\n\nA review - one vs. two mean test\n\n\n\n\nTable 2: Inference types - mean\n\n\n\n\n\n\nOne mean test\nTwo mean test\n\n\n\n\nComparing the mean of your sample to some statement about the world\nComparing the mean of one part of your sample to another part of your sample\n\n\nNull hypothesis: based on some belief we have about the general population, i.e. students sleep 7.03 hours\nNull hypothesis: no difference between groups\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nTable 3: Inference example\n\n\n\n\n\n\n\n\n\n\nOne mean test\nTwo mean test\n\n\n\n\nH0H_0: Our sample mean of hours of sleep is the same as all students in the world\nH0H_0: The sample mean of male students hours slept is the same as the mean of female students hours slept\n\n\nHaH_a: Our sample mean is different than the world’s population mean\nHaH_a: The sample mean of female students is different than the sample mean of male students"
  },
  {
    "objectID": "lectures/lecture.4.1.html#a-confidence-interval-for-the-difference-between-two-meansproportions",
    "href": "lectures/lecture.4.1.html#a-confidence-interval-for-the-difference-between-two-meansproportions",
    "title": "Lecture 4.1 - Comparing Groups",
    "section": "A confidence interval for the difference between two means/proportions",
    "text": "A confidence interval for the difference between two means/proportions\n\nDifference between means/proportions: standard error\n\nWant to find the SESE for y‾1−y‾2\\bar{y}_1-\\bar{y}_2\nStart with theoretical properties:\n\nSD(y‾1−y‾2)=Var(y‾1)+Var(y‾2)SD(\\bar{y}_1-\\bar{y}_2) = \\sqrt{Var(\\bar{y}_1) + Var(\\bar{y}_2)}\nSD(y‾1−y‾2)=σ12n1+σ22n2SD(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}\n\nDon’t know the population σ\\sigma for each subsample, so use the sample SDSDs as before\n\nSE(y‾1−y‾2)=s12n1+s22n2SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\nExample: proportions\n\n658 male passengers on the Titanic; 135 survived\n388 female passengers on the Titanic; 292 survived\nSD(p̂1−p̂2)=p1q1n1+p1q2n2SD(\\hat{p}_1-\\hat{p}_2) = \\sqrt{\\frac{p_1q_1}{n_1} + \\frac{p_1q_2}{n_2}}\nSE(0.205−0.753)=0.205×0.795658+0.753×0.247388SE(0.205-0.753) = \\sqrt{\\frac{0.205\\times0.795}{658} + \\frac{0.753\\times0.247}{388}}\nSE(0.205−0.753)=0.00024+0.000479SE(0.205-0.753) = \\sqrt{0.00024 + 0.000479}\nSE(0.205−0.753)=0.0269SE(0.205-0.753) = 0.0269\n\n\n\nConfidence interval\n\nWhat is the 95% confidence interval of the difference in means?\ny‾1−y‾2±Criticalvalue×SE\\bar{y}_1-\\bar{y}_2\\pm Critical value \\times SE\n−0.548±z*×0.0269-0.548\\pm z^* \\times 0.0269\n−0.548±1.96×0.0269-0.548\\pm 1.96 \\times 0.0269\n−0.548±0.0528-0.548\\pm 0.0528\n\nWhat can you conclude from this - how can you state the results? What are some factors that are omitted?\n\n\n\n\n\n\n\n\n\n\nCI for the difference between two proportions/means\n\nFirst find two-sample zz/tt interval for the difference in means\nThen apply two-sample zz/tt test\nInterval looks like others we have seen\n\ny‾1−y‾2±ME\\bar{y}_1-\\bar{y}_2\\pm ME\nME=t*/z*×SE(y‾1−y‾2)ME = t^*/z^*\\times SE(\\bar{y}_1-\\bar{y}_2)\n\nUses the zz model (proportion) or Student’s tt model (mean)\nThe degrees of freedom for tt are complicated, so just use a computer\n\n\n\nSampling distribution for the difference between two means\n\nWhen the conditions are met, the sampling distribution of the standardized sample difference between the means of two independent groups:\n\nt=(y‾1−y‾2)−(μ1−μ2)SE(y‾1−y‾2)t = \\frac{(\\bar{y}_1-\\bar{y}_2) - (\\mu_1 - \\mu_2)}{SE(\\bar{y}_1-\\bar{y}_2)}\n\nUses the Student’s tt model\nDegrees of freedom are found with a special formula\nThink carefully here about what we are modeling\n\n\n\nAssumptions\n\nIndependence assumption:\n\nWithin each group, individual responses should be independent of each other.\nKnowing one response should not provide information about other responses.\n\nRandomization condition:\n\nIf responses are selected with randomization, their independence is likely.\n\nIndependent Groups Assumption\n\nResponses in the two groups are independent of each other.\nKnowing how one group responds should not provide information about the other group.\n\n\n\n\nAssumptions continued\n\nNearly normal condition\n\nCheck this for both groups\nA violation by either one, violates the condition\nn&lt;15n &lt; 15 in either group: should not use these methods if the histogram or Normal probability plot shows severe skewness\nnn closer to 40 for both groups: mildly skewed histogram is OK\nn&gt;40n &gt; 40 for both groups: Fine as long as no extreme outliers or extreme skewness\n\n\n\n\nConfidence interval formally\n\nWhen the conditions are met, the confidence interval for the difference between means from two independent groups is\n\n(y‾1−y‾2)±tdf*×SE(y‾1−y‾2)(\\bar{y}_1-\\bar{y}_2)\\pm t^*_{df}\\times SE(\\bar{y}_1-\\bar{y}_2)\nwhere SE(y‾1−y‾2)=s12n1+s22n2SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\nCritical value tdf*t^*_{df} depends on confidence level CC"
  },
  {
    "objectID": "lectures/lecture.4.1.html#the-two-sample-t-test-testing-for-the-difference-between-two-means",
    "href": "lectures/lecture.4.1.html#the-two-sample-t-test-testing-for-the-difference-between-two-means",
    "title": "Lecture 4.1 - Comparing Groups",
    "section": "The two sample tt test: testing for the difference between two means",
    "text": "The two sample tt test: testing for the difference between two means\n\nA two sample tt test for difference between means\n\nConditions same as two-sample t-interval\n\nH0:μ1−μ2=Δ0H_0: \\mu_1-\\mu_2 = \\Delta_0 (Δ0\\Delta_0 usually 00)\n\nWhen the conditions are met and the null hypothesis is true, use the Student’s tt model to find the pp value.\n\nt=(y‾1−y‾2)−Δ0SE(y‾1−y‾2)t = \\frac{(\\bar{y}_1-\\bar{y}_2) - \\Delta_0}{SE(\\bar{y}_1-\\bar{y}_2)}\nSE(y‾1−y‾2)=s12n1+s22n2SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\n\nStep by step example\n\nIs there a difference in housing price depending on if the house has a view?\n\nThink →\\rightarrow\n\nPlan: I have housing prices from many thousands of houses in King County, assumed to have been sampled randomly.\nHypotheses\nH0:μw−μnotw=0H_0: \\mu_w-\\mu_{notw}=0\nHa:μw−μnotw≠0H_a: \\mu_w-\\mu_{notw}\\ne0\n\n\n\n\n\nStep by step example\n\nThink →\\rightarrow\n\nMean price of house with a view: 1772071, mean price no view: 1139608\nModel:\n\nRandomization Condition: Subjects assigned to treatment groups randomly?\nIndependent Groups Assumption: Sampling method gives independent groups?\nNearly normal condition: Histograms are reasonably unimodal and symmetric?\nThe assumptions and conditions are reasonable?\n\n\n\n\n\n\n\n\n\n\n\nAfter analyzing these assumptions , are we justified in using the Student’s t-model to perform a two-sample t-test?\n\n\n\n\n\n\n\n\n\n\nStep by step example\n\nShow\n\nMechanics\n\nMean price of house with a view: 1772071\nMean price no view: 1139608\nSD of view: 1128502\nSD no view: 823799\nnn of view: 433\nnn of no view 21504\n\n\n\nWhat is the formula we should use in the next step?\n\n\n\n\n\n\n\n\n\n\nStep by step example\n\nShow\n\nMechanics\n\nsolve for the SE: SE(y‾1−y‾2)=s12n1+s22n2SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\nsolve for the SE: SE(y‾1−y‾2)=1128502433+82379921504SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{1128502}{433} + \\frac{823799}{21504}}\nsolve for the SE:\nfind tt score: t=(y‾1−y‾2)−Δ0SE(y‾1−y‾2)t = \\frac{(\\bar{y}_1-\\bar{y}_2) - \\Delta_0}{SE(\\bar{y}_1-\\bar{y}_2)}\nfind tt score: t=(1772071−1139608)−054523t = \\frac{(1772071-1139608) - 0}{54523}\nfind tt score: t=11.6t = 11.6\nfind pp value: can use table with df=433df=433, p=0p = 0\n\n\n\n\n\nStep by step example\nAlternatively, we can use the built-in tt test function\n\nShow\n\n\n\n\n\n\n\n\n\nWhat can we conclude, based on the results of this tt test? What are some assumptions of this tt test that may be violated?\n\n\n\n\n\n\n\n\n\n\nStep by step example\n\nTell →\\rightarrow\n\nConclusion: the pp value = 0 is less than the critical value\nIf there were no difference in the mean prices, then a difference this large would occur 1 times in millions of times\nToo rare to believe happened by chance? Yes\nReject H0H_0? Yes\nConclude that houses with a view are more expensive than regular houses? Yes"
  },
  {
    "objectID": "lectures/lecture.4.1.html#experiments",
    "href": "lectures/lecture.4.1.html#experiments",
    "title": "Lecture 4.1 - Comparing Groups",
    "section": "Experiments",
    "text": "Experiments\n\nIndependence\n\nIndependence assumption:\n\nWithin each group, individual responses should be independent of each other.\nKnowing one response should not provide information about other responses.\n\nRandomization condition:\n\nIf responses are selected with randomization, their independence is likely.\n\nIndependent Groups Assumption\n\nResponses in the two groups are independent of each other.\nKnowing how one group responds should not provide information about the other group.\n\n\n\n\nThe importance of the counterfactual\n\nFor causal inference, one should ask the counter-factual question, for those who received “treatment”, what would have happened to them if they hadn’t been treated?\nThat is, we only observe one state of reality (had more vegetables), but we want to know the DIFFERENCE the treatment had on the person by asking what would have happened if the DID NOT receive the treatment\n\n\n\nThe importance of the counterfactual\n\nMore formally, we are interested in the difference the treatment has on the response variable (Health)\nOn a child (y1y_1)that did receive more vegetables, we want to consider the case of what would have happened if they had NOT had the vegetables and find the treatment effect\n\nOr, y1t−y1cy_1^t- y_1^c = treatment effect (tt denoting treatment; cc denoting control)\nNote that y1ty_1^t is observed, but y1cy_1^c is not.\n\nThe problem is one of missing data – how to estimate y1cy_1^c?\n\n\n\nComparability problems\n\nIf subjects who receive treatment and those who do not are different in some important characteristics, we have selectivity bias – e.g. higher SES children were more likely to be in the vegetable treatment group\n\nViolates the independent group assumption because if rich children are more likely to be in the “eats vegetables” group we know that the observed value of the response variable, heathheath, is likely to be higher\nKnowing which group they are in gives us some knowledge of what their observed value of yy will be\n\nOften called “omitted variable bias.”\nBig problem in observational studies – many variables are probably not present that we’d like to know\nWhat are some omitted variables that might bias our finding that houses on the view have a higher price than houses not on the view?\n\n\n\n\n\n\n\n\n\n\n\nExperiments\n\nExperiments solve the omitted variable bias\nRandom assignment of treatment and control status ensures that subjects differ ON AVERAGE only in the treatment they receive\nWe can then compute the Average Treatment Effect (ATE) of being in the control vs. treatment\nA tt test between treatment and control group will therefore be accurate\nIn observational studies, it is very rare to be able to guarantee that assignment to the two groups are independent of the response variable.\n\nIf there are important other omitted variables that influence assignment to the two groups, need to control for these omitted variables via a multiple regression\n\n\n\n\nDrawbacks of experiments\n\nLack of generalizability – Often done on college students or in contrived settings (external validity)\nCost – very expensive to run a full experiment\nEthics – why shouldn’t we give positive treatments to everyone?\nMechanically complicated\n\nDifficult to ensure proper randomization\nDifficult to design appropriate treatments\nDifficult to develop appropriate measurements\n\n\n\n\nMultiple regression\n\nMultiple Regression\n\nAttempts to control for, or estimate the treatment effect, for each variable included, INDEPENDENT of the other variables\nHow sure are we of the treatment effect?\ntt test of the slope of the regression line\nNull hypothesis is that treatment variable (“eats vegetables”) makes no difference on response variable\nIf slope is non-zero, it indicates that differences in treatment produce differences in response variable (increase education →\\rightarrow increase in wages)"
  },
  {
    "objectID": "lectures/lecture.2.1.html#exercise-1",
    "href": "lectures/lecture.2.1.html#exercise-1",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn your pairs, try to think of two variables that, in the real world, that might have this correlation for each of the following correlations. Try to think of a few examples for each correlation.\n\n0.95\n0.75\n0.5\n0.25\n0.0\n-0.25\n-0.5\n-0.75\n-0.95\n\nPick a few of these and draw by hand what you expect these graphs to look like."
  },
  {
    "objectID": "lectures/lecture.2.1.html#exercise-2",
    "href": "lectures/lecture.2.1.html#exercise-2",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\nSeattle"
  },
  {
    "objectID": "lectures/lecture.2.1.html#viewing-an-example-relationship",
    "href": "lectures/lecture.2.1.html#viewing-an-example-relationship",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Viewing an example relationship",
    "text": "Viewing an example relationship\n\nFirst, what is our expectation about the relationship between bedrooms and square feet?\nDirection?\nForm?\nStrength?\nOutliers?"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---direction",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---direction",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - direction",
    "text": "Bedrooms and square feet - direction\nA scatterplot is the easiest way to check for direction. In this case, the direction is obvious"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---form",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---form",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - form",
    "text": "Bedrooms and square feet - form"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---strength",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---strength",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - strength",
    "text": "Bedrooms and square feet - strength\n\n\n\n\n\n\n\n\n\nCorrelation as a measure of strength\n\n\n\n\n\n\n\n\nThis correlation is a little weaker than perhaps what we expected\n\nIn general, mechanically generated processes with little noise can have very high correlations\nMost correlations of social or real world processes rarely have above moderate correlation due to noise"
  },
  {
    "objectID": "lectures/lecture.2.1.html#bedrooms-and-square-feet---outlier",
    "href": "lectures/lecture.2.1.html#bedrooms-and-square-feet---outlier",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Bedrooms and square feet - outlier",
    "text": "Bedrooms and square feet - outlier\nAgain, we do not have a rule for selecting outliers other than to observe them on the scatterplot. In this case, there is one very obvious value far from other values\n\n\n\n\n\n\n\n\nTo investigate if this outlier matters, we can check some other values of the observation.\n\n\n\n\n\n\n\n\nWhat kind of outlier do you think this is? Why?\n\nOutlier - actual observation\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\n\n\n\nHouse details\n\n\n\n\nData with no outlier\n\n\n\n\n\n\n\n\n\n\nDescribing the association\n\nDirection - positive\nForm - linear\nStrength - moderate/strong\nOutliers - one possible\n\nOutlier:"
  },
  {
    "objectID": "lectures/lecture.2.1.html#does-bed-and-square-feeet-relationship-match-expectations",
    "href": "lectures/lecture.2.1.html#does-bed-and-square-feeet-relationship-match-expectations",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Does bed and square feeet relationship match expectations?",
    "text": "Does bed and square feeet relationship match expectations?\n\nSeems to, more or less, the larger the house, the more bedrooms, so the relationship is positive\nThe relationship is fairly linear, indicating a strong relationship\nRelationship is moderate\nOutliers don’t seem like it is a problem\n\nHowever….\n\nWhat are some possible lurking variables that influence the relationship of bedrooms and bathrooms?"
  },
  {
    "objectID": "lectures/lecture.2.1.html#relationship-reexpressed",
    "href": "lectures/lecture.2.1.html#relationship-reexpressed",
    "title": "Lecture 2.1 - Association and correlation",
    "section": "Relationship reexpressed?",
    "text": "Relationship reexpressed?\nOne final issue to consider is if this relationship should be reexpressed - made more linear.\n\n\n\n\n\n\n\n\n\nSeems clearer\n\n\nYour turn\nWith your partner, develop some expectations about some of the variables in the kc.houses dataset might be related.\nVariables:\n\n\n\n\n\n\n\n\nWhat to do with your partner:\n\nWrite down an interesting question we think we might be able to answer by examining a relationship in this dataset\nChoose the variables that you think might be able to answer this question\nWrite down what you expect the relationship to be between these two variables based on any prior knowledge\nDecide which variable is the response variable and which is the predictor variable\nMake a scatterplot using one of the codeblocks in the previous section and identify the features of the association"
  },
  {
    "objectID": "lectures/lecture.2.4.html#basic-multiple-regression-interpretation",
    "href": "lectures/lecture.2.4.html#basic-multiple-regression-interpretation",
    "title": "Multiple Regression",
    "section": "Basic multiple regression interpretation",
    "text": "Basic multiple regression interpretation\n\nHouse prices\n\n\n\n\n\n\n\n\n\n\nWhen linear regression is not enough\n\n\\(R^2 = 0.278%\\) for sqft and sale_price\n27.8% of the variation in Price is accounted for\nWhat about the other 72%?\nCould include other lurking variables such as size of the lot a house is on - more land, higher cost right?\nA regression with two or more predictor variables is called a multiple regression.\n\n\n\nWhat is multiple regression?\n\nFor a simple regression, with one independent variable, the least squares line makes residuals as small as possible.\nFor multiple regression, the regression equation still makes the residuals as small as possible.\nNo longer trying to create a line though – instead a multidimensional hyperplane!\nCalculations difficult.\n\n\n\nCheck grade and sale_price\n\n\n\n\n\n\n\n\nWhat do you think will happen to the coefficient on grade when we add sqft?\n\n\n\n\n\n\n\n\n\n\nAdding both terms\n\n\n\n\n\n\n\n\n\n\nThe results\n\n\\(R^2=0.3051\\)\n\\(s_e=696500\\)\nCoefficient:\n\n\\(price = -872678 + 329.513sqft\\_livingspace + 177278grade\\)\n\n\nHow would you interpret this model and the diagnostic statistics?\n\n\n\n\n\n\n\n\n\n\nFurther investigation"
  },
  {
    "objectID": "lectures/lecture.2.4.html#what-is-different-in-multiple-regression",
    "href": "lectures/lecture.2.4.html#what-is-different-in-multiple-regression",
    "title": "Multiple Regression",
    "section": "What is different in multiple regression?",
    "text": "What is different in multiple regression?\n\nMeaning of coefficients has changed in a subtle way.\nIs an extraordinarily versatile calculation, underlying many widely used statistics methods.\nOffers a glimpse into statistical models that use more than two quantitative variables.\nModels that use several variables can be a big step toward realistic and useful modeling of complex phenomena and relationships\n\n\nMultiple regression - coefficients\n\nCan’t assume coefficients will stay the same\nCoefficients change\nOften in unexpected ways\nEven changing signs\nBe alert for a change in value\nBe alert for a change in meaning\n\n\n\nMultiple regression model\n\nNo simple relationship between \\(y\\) and \\(x_j\\), yet \\(b_j\\) in a multiple regression may be quite different from zero\nStrong two-variable relationship between \\(y\\) and \\(x_j\\), yet \\(b_j\\) in a multiple regression to be almost zero\nStrong two-variable relationship between \\(y\\) and \\(x_j\\), yet \\(b_j\\) an be opposite in sign in a multiple regression\nEasy to extend the model with more predictors\nResiduals \\(e = y - \\hat{y}\\)"
  },
  {
    "objectID": "lectures/lecture.2.4.html#assumptions",
    "href": "lectures/lecture.2.4.html#assumptions",
    "title": "Multiple Regression",
    "section": "Assumptions",
    "text": "Assumptions\n\nThree key assumptions\n\nLinearity assumption (straight enough condition)\nNo pattern in residuals (outliers, straight enough condition)\nEqual variance assumption (does the plot thicken?)\n\n\n\nLinearity assumption\n\nStraight Enough Condition\n\nWe must check the scatterplot for each of the predictor variables vs. the response variable\nDo not need the scatterplots to show any discernible slope, but should be reasonably straight\nCannot have bends, or other nonlinearity\nCan be easier to look at the plot of residuals\n\n\n\n\n\n\n\n\n\n\n\n\nCheck the residual\n\nErrors have a distribution that is:\n\nUnimodal\nSymmetric\nWithout outliers\n\nLook at histogram of residuals\nAssumption is less important as sample size increases\n\n\n\n\n\n\n\n\n\n\n\nEqual variance assumption\n\nSame variability of the errors for all values of each predictor\nDoes the Plot Thicken? Condition: the spread around the line must be nearly constant.\nBe alert for “fan” shaped pattern\nOr other tendency for variability to grow or shrink in one part of the scatterplot\n\n\n\n\n\n\n\n\n\n\n\nDecision loop\n\nStraight Enough Condition: scatterplots of y-variable against each x-variable\n\nIf straight enough, fit multiple regression model\n\nHow were data collected? Random? Represent identifiable population? Time? check independence\nFind the residuals and predicted values.\nScatterplot of the residuals against predicted values: patternless, no bends, no thickening\nHistogram of residuals: unimodal, symmetric, without outliers\nIf conditions check out, interpret regression model, and make predictions."
  },
  {
    "objectID": "lectures/lecture.2.4.html#partial-residual-plots",
    "href": "lectures/lecture.2.4.html#partial-residual-plots",
    "title": "Multiple Regression",
    "section": "Partial residual plots",
    "text": "Partial residual plots\nOne of the best ways to check the linearity condition is with a partial residual plot. This plot displays the relationship between the predictor variable and the response variable after removing all of the variance of the other variables in the explanatory variable.\n\nHow to check variables individually\n\nChecked overall equation for weirdness in residuals\nWhat about each individual variable’s contribution to the regression?\nPartial residual plot to the Rescue!\nLook at plot to judge whether its form is straight enough.\n\n\n\nPartial residual plots\n\n\n\n\n\n\n\n\n\n\nMeaning of a partial residual plot\n\nLeast squares line fit to plot has slope equal to the coefficient the plot illustrates.\nResiduals are same as final residuals of multiple regression\n\nJudge strength of estimation of the plot’s coefficients\n\nOutliers seen the same as they would appear in a simple scatterplot\nDirection corresponds to the sign of multiple regression coefficient"
  },
  {
    "objectID": "lectures/lecture.2.4.html#indicator-variables",
    "href": "lectures/lecture.2.4.html#indicator-variables",
    "title": "Multiple Regression",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nWages\n\nIndicator variables are for when we want to include categorical variables in our regression\n\nIn a union vs. not in a union\nOften coded at 1=true 0=false, but that’s just convention, doesn’t really matter (remember, units don’t matter for regression)\n\nRegression equation\n\n\\(wages = b_0 + b_1exp + b_2union\\)\n\n\n\n\nWages\n\n\n\n\n\n\n\n\n\n\nSlopes of lines\n\n\n\n\n\n\n\n\n\n\nPredict some values\n\nEquation: \\(wages = 747.5634 + 8.2430exp + -77.7134union\\)"
  },
  {
    "objectID": "lectures/lecture.2.4.html#interaction-terms",
    "href": "lectures/lecture.2.4.html#interaction-terms",
    "title": "Multiple Regression",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nInteraction effects\n\nWhat if lines are not roughly parallel?\nIndicator variable that is 0 or 1 shifts line up or down.\n\nCan’t change slope\nWorks only when same slope just different intercepts\n\n\n\n\nAdjusting for different slopes\n\nIntroduce another constructed variable\nThe one is the product of an indicator variable and the predictor variable\nCoefficient of this constructed interaction term gives adjustment to slope, \\(b_1\\), to be made for the individuals in the indicated group.\n\n\n\nAdjusting for different slopes\n\n\n\n\n\n\n\n\n\n\nDifferent slopes for wages\n\n\n\n\n\n\n\n\n\n\nPredict some values\n\nEquation: \\(wages = 710.7896 + 10.1421exp + 28.9884union + -5.2755union*exp\\)"
  },
  {
    "objectID": "lectures/lecture.2.4.html#footnotes",
    "href": "lectures/lecture.2.4.html#footnotes",
    "title": "Multiple Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCredit to: https://crosscut.com/opinion/2020/11/washington-state-housing-question-and-answer↩︎"
  },
  {
    "objectID": "lectures/lecture.4.2.html#moving-from-description-to-inference",
    "href": "lectures/lecture.4.2.html#moving-from-description-to-inference",
    "title": "Returning to Regression",
    "section": "Moving from description to inference",
    "text": "Moving from description to inference\n\nReturning to regression\n\n\n\n\n\n\n\n\nHow would you interpret all parts of this model except the pp value, tvaluet value and Std.ErrorStd. Error?\n\n\n\n\n\n\n\n\n\n\nModel\n\nThe equation of the least squares line mpĝ=46.32−15.35×weight\\hat{mpg} = 46.32−15.35\\times weight\nSlope of -15.35 indicates that the miles per gallon of cars is on average 15.35 less for each additional ton the car weighs.\nHow useful is the model?\nSlope and intercept are descriptions of data - want to know how certain we are of this slope estimate\nWant to understand what it can tell us beyond the 400 cars in the study\nConstruct CI, test hypotheses about slope, intercept\n\n\n\nSample vs. model regression\n\nSample:\n\nŷ=b0+b1x\\hat{y}=b_0 + b_1x\nThis gives a prediction for yy based on the sample.\n\nModel:\n\nμy=β0+β1x\\mu_y = \\beta_0 + \\beta_1x\nb0b_0 = y-intercept for the model\nb1b_1 = slope for the model\nThe model assumes that for every value of xx, the mean of the yy’s lies on the line.\n\n\n\n\n\nSlope variability\n\n\nJust as with a mean, if we sample many times from a population, we will, by chance, get variation in our estimate of the slope of the relationship between the two variables.\n\n\n\n\n\n\n\n\nWe can then make a histogram of each of these slope estimates.\n\n\n\n\n\n\n\n\nIn expectation, what shape should we expect the distribution of slope estimates to conform to? Why?\n\n\n\n\n\n\n\n\n\n\nErrors\n\nThe model predicts the mean of yy for each xx, but misses the actual individual values of yy\n\nμy=β0+β1x\\mu_y = \\beta_0 + \\beta_1x\n\nThe error, e, is the amount the line misses the value of yy.\n\nŷ=β0+β1x+e\\hat{y} = \\beta_0 + \\beta_1x+e\n\nThis new equation gives the exact value of each of the yy’s."
  },
  {
    "objectID": "lectures/lecture.4.2.html#review-of-regression-conditions",
    "href": "lectures/lecture.4.2.html#review-of-regression-conditions",
    "title": "Returning to Regression",
    "section": "Review of regression conditions",
    "text": "Review of regression conditions\n\nStraight enough condition\n\nStraight enough condition\n\nDoes the scatterplot look relatively straight?\n\nDon’t draw the line. It can fool you.\n\nLook at scatterplot of the residuals.\n\nShould have horizontal direction\nShould not have a pattern\n\nIf straight enough, check the other assumptions.\nIf not straight, stop or re-express.\n\n\n\n\n\n\n\n\n\n\n\n\nIndependence assumption\n\nErrors (ee’s) must be independent of each other.\n\nCheck residuals plot.\n\nShould not have clumps, trends, or a pattern.\nShould look very random.\n\nTo make inferences about the population, the sample must be representative.\nFor x=timex = time, plot residuals vs. residuals one step later.\n\nShould look very random\n\n\n\nWe can partially check the independence assumption by viewing the residuals.\n\n\n\n\n\n\n\n\n\n\nEqual variance assumption\n\nVariability of yy same for all yy\n\nDoes the plot thicken? condition: Spread along the line should be nearly constant.\n\n“Fan shape” is bad.\n\nStandard deviation of residuals, ses_e, will be used for CI and hypothesis tests.\n\nThis requires same variance for each xx.\n\n\n\nThe equal variance assumption can be analyzed by also viewing the residual plot.\n\n\n\n\n\n\n\n\n\n\nNormal population assumption\n\nAs with the tt and zz test, we are relying on the Central Limit Theorem that our sampling distribution of our statistic being normally distributed\n\nStatistic in this case is not mean but regression coefficient\n\nTo meet this condition, the errors for each fixed xx must follow the Normal model.\n\nGood enough to use the Nearly Normal Condition and the Outlier condition for the predictors.\nLook at the histograms.\nWith large sample sizes, the Nearly Normal Condition is usually satisfied.\n\n\nThe normal population assumption can be partially checked by viewing a histogram of the residuals.\n\n\n\n\n\n\n\n\nDo these plots indicate that the conditions for regression are satisfied?\n\n\n\n\n\n\n\n\n\n\nWhich comes first, the conditions or the residuals?\n\nCheck Straight Enough Condition with scatterplot.\nFit regression, find predicted values and residuals.\nMake scatterplot of residuals and check for thickening, bends, and outliers.\nFor data measured over time, use residuals plot to check for independence.\nCheck the Nearly Normal Condition with a histogram and Normal plot of residuals.\nIf no violations, proceed with inference.\n\n\nNote: Stop if at any point there is a violation."
  },
  {
    "objectID": "lectures/lecture.4.2.html#sampling-distribution-of-models",
    "href": "lectures/lecture.4.2.html#sampling-distribution-of-models",
    "title": "Returning to Regression",
    "section": "Sampling distribution of models",
    "text": "Sampling distribution of models\n\nSample to sample variation of the slope and intercept\n\nNull hypothesis (usually) - regression slope = 0\n\nH0:b1=0H_0: b_1 = 0\nHa:b1≠0H_a: b1 \\neq 0\n\npp values in the table come from tests of this hypothesis for each coefficient.\nTo calculate pp value, we need to describe our sampling distribution.\nThe mean of the sampling distribution of the regression slope will be 0, from the null hypothesis.\nWe assume shape of the sampling distribution will be normal (from the Central Limit Theorem)\nStandard errors (or our estimate of the standard deviation of the sampling distribution) must come from the data.\nEach sample of 400 cars will produce its own line with slightly different b0b_0’s and b1b_1’s.\n\n\n\nSpread around the line\n\nLess scatter along the line →\\rightarrow slope more consistent\nResidual standard deviation, ses_e, measures this scatter.\n\n\n\n\nSpread around the line\n\n\n\nLess scatter around the line, smaller the residual standard deviation and stronger the relationship between xx and yy\nSome assess strength of regression by looking at ses_e\nIt has the same units as yy\nTells how close data are to the our model.\nr2r^2 is proportion of the variation of yy accounted for by xx\n\n\n\n\n\n\n\n\n\n\nLarger sxs_x (SD in xx) →\\rightarrow more stable regression\n\n\n\n\nSpread in x’s\n\n\n\n\nSample size\n\nLarger sample size →\\rightarrow more stable regression\n\n\n\n\nSample size stability\n\n\n\n\nStandard error for the slope\n\nThree aspects of the scatterplot, then, affect the standard error of the regression slope:\n\nSpread around the model: ses_e\nVariation among the xx values.\nSample size: nn\n\nThese are in fact the only things that affect the standard error of the slope.\n\n\n\nStandard error formally\n\nThe standard error for the slope parameter is estimated by:\n\nSE(b1)=sen−1sxSE(b_1) = \\frac{s_e}{\\sqrt{n-1}s_x} where se=∑(y−ŷ)2n−2s_e = \\sqrt{\\frac{\\sum{(y-\\hat{y})^2}}{n-2}}\n\nWe can then calculate how many tt units our slope estimate is from the null hypothesis by:\n\nt=b1−β1SE(b1)t=\\frac{b_1-\\beta_1}{SE(b_1)}\nThe tt follows Student’s tt model with df=n−2df=n-2\n\nDon’t need to remember the SE formula, just note that the process is exactly the same as for the mean\n\nFind the number of tt units the slope is away from the mean\nUse that tt score to find a pp value of how likely it would be to observe a difference in slopes that larger or larger from the null just by chance"
  },
  {
    "objectID": "lectures/lecture.4.2.html#inference-for-regression",
    "href": "lectures/lecture.4.2.html#inference-for-regression",
    "title": "Returning to Regression",
    "section": "Inference for regression",
    "text": "Inference for regression\n\nExample - wages\n\nIt has long been known women earn less than men, and this fact is often taken as evidence of discrimination\nHow to test with statistics though?\n\n\n\n\nGender wage gap\n\n\n\n\nWhat about a tt test?\n\nSlam dunk case right??\n\n\n\n\n\n\n\n\n\nWhat are some reasons why this tt test might be misleading?\n\n\n\n\n\n\n\n\n\n\nOther factors\n\nWomen dropping out of the labor force\nImpact of high earning men influencing the calculation (outliers)\nWomen’s career choice\nDifferences in educational attainment\nOther factors?\n\n\n\n1. Check straight enough condition\nTwo-way scatterplots\n\n\n\n\n\n\n\n\nHistogram of wage\n\n\n\n\n\n\n\n\nHistogram of log(wage)\n\n\n\n\n\n\n\n\nDo these plots suggest the straight enough condition has been met?\n\n\n\n\n\n\n\n\n\n\n2. Fit the regression\n\n\n\n\n\n\n\n\nInterpret this regression table.\n\n\n\n\n\n\n\n\n\n\n3. Check the residuals\n\n\n\n\n\n\n\n\n\n\n4. Check the Nearly Normal condition of the residuals\n\n\n\n\n\n\n\n\nDo the residual plots suggest the conditions of the residuals have been met?\n\n\n\n\n\n\n\n\n\n\n5. Proceed with inference\n\n\n\n\n\n\n\n\nHow should we understand the coefficient of sexsex from the regression compared to the simple difference of means between male and female?\n\n\n\n\n\n\n\n\n\n\nCollinearity\n\nVariables are said to be collinear when their correlation is very high\n\nEx: HDI score and GDP/capita\n\nWhat the regression model is trying to do is apportion the amount of responsibility of each predictor independent of the other predictors\n\ni.e. What is the impact of female INDEPENDENT of all other predictors?\n\n\n\n\nCollinearity\n\nWhen predictor is collinear\n\nCoefficient surprising: unanticipated sign or unexpectedly large or small value\nSE of coefficient can be inflated, leading to smaller tt statistic, larger pp value\n\nWhat to do?\n\nRemove some of the predictors\n\nSimplifies model, improves tt statistic of slope (usually)\n\nKeep those most reliably measured, least expensive to find, or ones that are politically important"
  },
  {
    "objectID": "rmanual.html",
    "href": "rmanual.html",
    "title": "R Manual",
    "section": "",
    "text": "credit to Maisie Zhang, the first Stats 101 head tutor, for compiling this guide."
  },
  {
    "objectID": "rmanual.html#install-software",
    "href": "rmanual.html#install-software",
    "title": "R Manual",
    "section": "Install software",
    "text": "Install software\nNote that you must install R before RStudio and both programs are required\nR statistical package\nR Studio user interface"
  },
  {
    "objectID": "rmanual.html#notes-on-use",
    "href": "rmanual.html#notes-on-use",
    "title": "R Manual",
    "section": "Notes on use",
    "text": "Notes on use\n\nCase: does differentiate between uppercase and lowercase\nVariable name: consists of letters, numbers and the dot or underline characters, and starts with a letter or the dot not followed by a number\nPunctuation: only English punctuation is accepted\nPound sign: # leads a comment line\nDollar sign: $ extracts elements by name from a named list and is often used as &lt;data.frame&gt;$&lt;column&gt;\nPercent sign: % is not accepted as percentage and percentage needs to be expressed as fraction\nAssignment operator: &lt;- (left arrow) assigns a value to a name\n\nExample: x &lt;- 2 + 3\n\nTo R, the result is a vector, even though for simple calculations the vector has only one element\nPipe operator: %&gt;% indicates that you are passing the result of one function to the next function. Requires the tidyverse package"
  },
  {
    "objectID": "rmanual.html#install-packages",
    "href": "rmanual.html#install-packages",
    "title": "R Manual",
    "section": "Install packages",
    "text": "Install packages\nR by default comes with many built-in tools however the real power of the program comes from the packages others have written to extend R.\nTo install a new package, there are two options:\n\nUse the command line:\n\ninstall.packages (\"ggplot2\")\n\nUse the user interface:\n\n\nClick Tools - Install Packages…\nType ggplot2 in Packages blank\nClick Install\n\nthen, on the command line, type:\nlibrary(ggplot2)\n\nYou only need to install a package once. However, each time you restart R, you must load the library again using the library() command"
  },
  {
    "objectID": "rmanual.html#keyboard-shortcuts-tips",
    "href": "rmanual.html#keyboard-shortcuts-tips",
    "title": "R Manual",
    "section": "Keyboard shortcuts & tips",
    "text": "Keyboard shortcuts & tips\n\nNavigate command history: Up / Down\nClear console: Ctrl + L\nChange font size: click Tools - Global Options… - Appearance - Editor Font size"
  },
  {
    "objectID": "rmanual.html#basic-functions",
    "href": "rmanual.html#basic-functions",
    "title": "R Manual",
    "section": "Basic functions",
    "text": "Basic functions\nUsually in this class we will be using dpylr package to do most calculations and data manipulation. But in some cases you may find it faster to use the base R functions.\nhelp() : the primary interface to the help systems\n\nexample: help(mean)\n\n\nA more convenient way is to type just ? and the function name. For example, help(mean) and ?mean work the same\n\nc(): combines values into a vector or list\n\nresult &lt;- c(1, 2)\n\nresult\n\n[1] 1 2\n\n\nmean(x, na.rm = FALSE): arithmetic mean\n\nmean(mtcars$mpg)\n\n[1] 20.09062\n\n\nsd(x, na.rm = FALSE): sample standard deviation which uses denominator n-1\n\nsd(mtcars$mpg)\n\n[1] 6.026948\n\n\n\nNote this function does NOT calculate population SD but the sample SD\n\nsummary(): for numeric variables, returns minimum, 1st quartile, median, mean, 3rd quartile, and maximum; for categorical variables, returns counts of all categories\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90"
  },
  {
    "objectID": "rmanual.html#piping",
    "href": "rmanual.html#piping",
    "title": "R Manual",
    "section": "Piping",
    "text": "Piping\nThe usual way to operate on data in this class is with pipes %&gt;%; a pipe will carry over the result of one calculation to the following calculation. You can create arbitrarily long computation chains that can save you a lot of typing over using the built-in R functions.\nTo do some of the calculations in the following sections, you can do instead:\nmean(mtcars$mpg) piped version:\n\nmtcars %&gt;% \n  summarize(mean = mean(mpg))\n\n      mean\n1 20.09062\n\n\nWe can combine the mean() and the sd() function in one command like the following:\n\nmtcars %&gt;% \n  summarize(mean = mean(mpg), sd = sd(mpg))\n\n      mean       sd\n1 20.09062 6.026948"
  },
  {
    "objectID": "rmanual.html#importing-datasets",
    "href": "rmanual.html#importing-datasets",
    "title": "R Manual",
    "section": "Importing datasets",
    "text": "Importing datasets\nAll datasets in this class are in .csv format (comma separated values). To import a .csv dataset, you need to:\n\nClick File - Import Dataset - From Text (base)…\nSelect the file\nMake sure Heading: Yes\nna.strings: if the dataset has any missing values, enter the code for missing values\nStrings as factors: make sure this is ticked\n\n\nIn R, categorical variables are represented as factors. So be careful here as some string values that are simply ID variables should not be classified as factors. There are a number of R commands that can convert between strings and factors later if you need to clean up your dataset after importing it."
  },
  {
    "objectID": "rmanual.html#removing-datasets",
    "href": "rmanual.html#removing-datasets",
    "title": "R Manual",
    "section": "Removing datasets",
    "text": "Removing datasets\nrm(mtcars)\n\nThe datasets we work with in R are not very large so I don’t recommend removing any datasets from memory. Once you delete it, there is no way to recover the dataset."
  },
  {
    "objectID": "rmanual.html#rename-a-dataset",
    "href": "rmanual.html#rename-a-dataset",
    "title": "R Manual",
    "section": "Rename a dataset",
    "text": "Rename a dataset\n\nmpgcars &lt;- mtcars"
  },
  {
    "objectID": "rmanual.html#dealing-with-factors",
    "href": "rmanual.html#dealing-with-factors",
    "title": "R Manual",
    "section": "Dealing with factors",
    "text": "Dealing with factors\nR stores categorical variables as a type of variable called a factor. This is different than variables stored as simply text - R will not perform any operations in variables stored simply as text.\nTo convert between text and factors, you can use the following commands:\n\nfruits &lt;- c(\"apple\", \"pear\", \"banana\")\n\n# Will not work properly because the variable is a text variable\nsummary(fruits)\n\n   Length     Class      Mode \n        3 character character \n\n# The factor command will convert it to a factor so R can interpret the contents\nsummary(factor(fruits))\n\n apple banana   pear \n     1      1      1 \n\n\nIt is not a commonly used function, but if for some reason you need to convert back to text, you can use as.character() or as.numeric()\n\nfruits &lt;- factor(fruits)\n\n# Converts a factor back to text. as.numeric() does the same if the factor names are numbers\nas.character(fruits)\n\n[1] \"apple\"  \"pear\"   \"banana\""
  },
  {
    "objectID": "rmanual.html#select-and-subset",
    "href": "rmanual.html#select-and-subset",
    "title": "R Manual",
    "section": "Select and subset",
    "text": "Select and subset\n\nSelecting columns\n\nmtcars %&gt;% \n  select(mpg)\n\n                     mpg\nMazda RX4           21.0\nMazda RX4 Wag       21.0\nDatsun 710          22.8\nHornet 4 Drive      21.4\nHornet Sportabout   18.7\nValiant             18.1\nDuster 360          14.3\nMerc 240D           24.4\nMerc 230            22.8\nMerc 280            19.2\nMerc 280C           17.8\nMerc 450SE          16.4\nMerc 450SL          17.3\nMerc 450SLC         15.2\nCadillac Fleetwood  10.4\nLincoln Continental 10.4\nChrysler Imperial   14.7\nFiat 128            32.4\nHonda Civic         30.4\nToyota Corolla      33.9\nToyota Corona       21.5\nDodge Challenger    15.5\nAMC Javelin         15.2\nCamaro Z28          13.3\nPontiac Firebird    19.2\nFiat X1-9           27.3\nPorsche 914-2       26.0\nLotus Europa        30.4\nFord Pantera L      15.8\nFerrari Dino        19.7\nMaserati Bora       15.0\nVolvo 142E          21.4\n\n\nTo select multiple columns you can do:\n\nmtcars %&gt;% \n  select(c(mpg, cyl))\n\n                     mpg cyl\nMazda RX4           21.0   6\nMazda RX4 Wag       21.0   6\nDatsun 710          22.8   4\nHornet 4 Drive      21.4   6\nHornet Sportabout   18.7   8\nValiant             18.1   6\nDuster 360          14.3   8\nMerc 240D           24.4   4\nMerc 230            22.8   4\nMerc 280            19.2   6\nMerc 280C           17.8   6\nMerc 450SE          16.4   8\nMerc 450SL          17.3   8\nMerc 450SLC         15.2   8\nCadillac Fleetwood  10.4   8\nLincoln Continental 10.4   8\nChrysler Imperial   14.7   8\nFiat 128            32.4   4\nHonda Civic         30.4   4\nToyota Corolla      33.9   4\nToyota Corona       21.5   4\nDodge Challenger    15.5   8\nAMC Javelin         15.2   8\nCamaro Z28          13.3   8\nPontiac Firebird    19.2   8\nFiat X1-9           27.3   4\nPorsche 914-2       26.0   4\nLotus Europa        30.4   4\nFord Pantera L      15.8   8\nFerrari Dino        19.7   6\nMaserati Bora       15.0   8\nVolvo 142E          21.4   4\n\n\nMore details on advanced select commands can be found here:\nSelecting columns documentation\n\n\nSelecting rows\n\nmtcars %&gt;% \n  filter(cyl==6)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C      17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\n\nAs with the column example, multiple criteria can be used:\n\nmtcars %&gt;% \n  filter(cyl==6 & hp==110)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n\nMore details on advanced filter commands can be found here:\nFiltering rows documentation"
  },
  {
    "objectID": "rmanual.html#recoding-variables",
    "href": "rmanual.html#recoding-variables",
    "title": "R Manual",
    "section": "Recoding variables",
    "text": "Recoding variables\nThe main dplyr verb for recoding a variable is mutate. With mutate, you can either simply rescale a variable or do more complex transformations, such as the following:\n\n# Weight is defined as 1000s of pounds; a ton is 2000 pounds\nmtcars &lt;- mtcars %&gt;% \n  mutate(tons = (wt / 2000) * 1000)\n\nA more complex transformation using the helper function case_when:\n\n# Note we need to convert the strings to a factor after the case_when call\nmtcars &lt;- mtcars %&gt;% \n  mutate(cartype = case_when(\n    tons &lt; 1 ~ \"light\",\n    tons &gt;= 1 & tons &lt; 2 ~ \"medium\",\n    tons &gt; 2 ~ \"heavy\"\n  )) %&gt;% \n  mutate(cartype = factor(cartype))\n\nMore details on advanced mutate commands can be found here:\nMutate documentation\nMore details on advanced case_when commands can be found here:\nCase when documentation"
  },
  {
    "objectID": "rmanual.html#missing-values",
    "href": "rmanual.html#missing-values",
    "title": "R Manual",
    "section": "Missing values",
    "text": "Missing values\nMany datasets have missing values (for a variety of reasons). It is always important to check if your dataset has any missing values (in R, these are stored by default as NA).\nTo check the total number of missing values, we can use the following:\n\nmtcars.missing &lt;- mtcars\n\n# Make the first observation missing\nmtcars.missing[1,1] &lt;- NA\n\ncolSums(is.na (mtcars.missing))\n\n    mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear \n      1       0       0       0       0       0       0       0       0       0 \n   carb    tons cartype \n      0       0       0 \n\n\nMost functions in R have a flag to remove missing values. If there is a missing value, the function will return an error if the flag is not set.\n\nmean(mtcars.missing$mpg)\n\n[1] NA\n\nmean(mtcars.missing$mpg, na.rm=TRUE)\n\n[1] 20.06129\n\n\n\nSometimes cases with missing values suggest that certain types of observations are not stored properly. It is always important to check if there are any patterns in the observations with missing values before removal."
  },
  {
    "objectID": "rmanual.html#helpful-summary-functions",
    "href": "rmanual.html#helpful-summary-functions",
    "title": "R Manual",
    "section": "Helpful summary functions",
    "text": "Helpful summary functions\n\nSummarizing data\nThe simpliest way to summarize data is the summary() command.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nTo develop more complex summary statistics, you can use longer piped structures from dplyr as follows:\n\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(mean = mean(wt), n = n())\n\n# A tibble: 3 × 3\n    cyl  mean     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     4  2.29    11\n2     6  3.12     7\n3     8  4.00    14\n\n\nTo view more samples of the summarize() function, see here:\nSummarize documetation\n\n\nFinding normal distribution & quantile information\nIf you want to find out details of the normal distribution, there are a suite of function in R that can report the quantile of a specific number of standard deviations from the center of a normal distribution or the reverse.\n\n# What is the amount of area under the normal curve cumulative up to +2 sd from the mean \npnorm(2, mean=0, sd=1)\n\n[1] 0.9772499\n\n# At how many standard deviations away from the mean is 97.5% of the area under the normal curve\nqnorm(0.975, mean=0, sd=1)\n\n[1] 1.959964\n\n\nIf you have an arbitrary distribution (that may not necessarily be normally distributed), you can find a quantile for a given percentage of area under the curve with the quantile() function.\n\n# generate a uniform distribution\ndist&lt;-runif(1000, min=0, max=1)\n\n# theoretically, the quantile should match the probability for a uniform distribution\nquantile(dist, probs=0.5)\n\n      50% \n0.5068224"
  },
  {
    "objectID": "rmanual.html#frequency-table-and-contingency-table",
    "href": "rmanual.html#frequency-table-and-contingency-table",
    "title": "R Manual",
    "section": "Frequency table and contingency table",
    "text": "Frequency table and contingency table\nCreating a frequency table is quite easy in R.\n\ntable(mtcars$cyl, mtcars$gear)\n\n   \n     3  4  5\n  4  1  8  2\n  6  2  4  1\n  8 12  0  2\n\naddmargins(table(mtcars$cyl, mtcars$gear))\n\n     \n       3  4  5 Sum\n  4    1  8  2  11\n  6    2  4  1   7\n  8   12  0  2  14\n  Sum 15 12  5  32\n\n\nSo is making a contingency table.\n\nprop.table(table(mtcars$cyl, mtcars$gear))\n\n   \n          3       4       5\n  4 0.03125 0.25000 0.06250\n  6 0.06250 0.12500 0.03125\n  8 0.37500 0.00000 0.06250\n\naddmargins(prop.table(table(mtcars$cyl, mtcars$gear)))\n\n     \n            3       4       5     Sum\n  4   0.03125 0.25000 0.06250 0.34375\n  6   0.06250 0.12500 0.03125 0.21875\n  8   0.37500 0.00000 0.06250 0.43750\n  Sum 0.46875 0.37500 0.15625 1.00000\n\n\nChanging the method of calculating the margins can be done with the margin option.\n\nprop.table(table(mtcars$cyl, mtcars$gear), margin=1)\n\n   \n             3          4          5\n  4 0.09090909 0.72727273 0.18181818\n  6 0.28571429 0.57142857 0.14285714\n  8 0.85714286 0.00000000 0.14285714\n\nprop.table(table(mtcars$cyl, mtcars$gear), margin=2)\n\n   \n             3          4          5\n  4 0.06666667 0.66666667 0.40000000\n  6 0.13333333 0.33333333 0.20000000\n  8 0.80000000 0.00000000 0.40000000"
  },
  {
    "objectID": "rmanual.html#histogram",
    "href": "rmanual.html#histogram",
    "title": "R Manual",
    "section": "Histogram",
    "text": "Histogram\nHistograms are quite easy using ggplot.\n\nggplot(mtcars, aes(x=wt)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of geom_histogram() can be found here:\nHistogram documetation\n\nRemember, histograms should only be use for quantitative data. Even if a categorical variable is numeric (cylinders, for example), you should represent that variable with a bar chart."
  },
  {
    "objectID": "rmanual.html#boxplot",
    "href": "rmanual.html#boxplot",
    "title": "R Manual",
    "section": "Boxplot",
    "text": "Boxplot\nBoxplots are useful if you want to compare the distribution of a variable across several different categories. For example, boxplots are a good way to compare the distribution of car weight by number of cylinders in a car.\n\nggplot(mtcars, aes(x=factor(cyl), y=wt)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nThe grouping variable x must be categorical in the box plot call\n\nDetails on how to modify the graphical parameters of geom_boxplot() can be found here:\nBoxplot documetation"
  },
  {
    "objectID": "rmanual.html#bar-chart",
    "href": "rmanual.html#bar-chart",
    "title": "R Manual",
    "section": "Bar chart",
    "text": "Bar chart\nBar charts are the best way to display the distribution of categorical variables. Since we generally don’t assign importance to the order or distance between categories, we can only simply show the count of each category independently.\n\nggplot(mtcars, aes(x=factor(cyl))) +\n  geom_bar()\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of geom_bar() can be found here:\nBar chart documetation"
  },
  {
    "objectID": "rmanual.html#qq-plot-in-ggplot",
    "href": "rmanual.html#qq-plot-in-ggplot",
    "title": "R Manual",
    "section": "QQ Plot in ggplot",
    "text": "QQ Plot in ggplot\nQQ plots are useful to determine if a distribution is normal shaped. While we won’t discuss it in this class, it may be helpful for certain projects you work on. Similar to other singe variable ggplots, you can create one as follows:\n\nggplot(mtcars, aes(sample=wt)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\n\n\n\n\nInformation on how to interpret these plots can be found in Chapter 5 of the textbok.\nDetails on how to modify the graphical parameters of geom_qq() can be found here:\nQQ plot documetation"
  },
  {
    "objectID": "rmanual.html#scatterplot-in-ggplot",
    "href": "rmanual.html#scatterplot-in-ggplot",
    "title": "R Manual",
    "section": "Scatterplot in ggplot",
    "text": "Scatterplot in ggplot\nScatterplots are the most common way to display a two-variable relationship and are one of the most common graphical displays.\n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nRemember that your response variable should always be on the y axis on a scatterplot\n\nIt is relatively common to add a line of best fit into a scatterplot to summarize the relationship between the two variables. You can do so by using the geom_smooth() function.\n\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE)\n\n\n\n\n\n\n\n\nThere are a number of different options for how to choose a best fit line, remember to select the one that you think best represents the relationship between the two variables.\nDetails on how to modify the graphical parameters of geom_point() and geom_smooth() can be found here:\nScatterplot documetation\nSmoother documetation"
  },
  {
    "objectID": "rmanual.html#correlation-in-ggcorrplot",
    "href": "rmanual.html#correlation-in-ggcorrplot",
    "title": "R Manual",
    "section": "Correlation in ggcorrplot",
    "text": "Correlation in ggcorrplot\nOne helpful graphical display to quickly summarize the relationship between many variables is a correlation plot. It displays the correlation between the variables you specify in your dataset. First, you need to work out which variables you want to include in the correlation matrix.\n\nmtcars.subset &lt;- mtcars %&gt;% \n  select(c(mpg, cyl, wt, disp))\n\n# use = complete.obs here in case of missing values\nmtcars.cor &lt;- cor(mtcars.subset, use=\"complete.obs\")\n\nmtcars.cor\n\n            mpg        cyl         wt       disp\nmpg   1.0000000 -0.8521620 -0.8676594 -0.8475514\ncyl  -0.8521620  1.0000000  0.7824958  0.9020329\nwt   -0.8676594  0.7824958  1.0000000  0.8879799\ndisp -0.8475514  0.9020329  0.8879799  1.0000000\n\n\nThen simply use the ggcorrplot() function.\n\nggcorrplot(mtcars.cor)\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of ggcorrplot() can be found here:\nggcorrplot documetation"
  },
  {
    "objectID": "rmanual.html#adding-features-to-ggplots",
    "href": "rmanual.html#adding-features-to-ggplots",
    "title": "R Manual",
    "section": "Adding features to ggplots",
    "text": "Adding features to ggplots\nYou can modify ggplots to add many different features. Below is a short list of basic things you can add but this is not by far a complete list.\n\nChange the colors and lines\n\nggplot (mtcars, aes(x = wt)) + \n  geom_histogram(fill = \"#FF6666\", alpha = 0.8, color = \"black\", linetype = \"dashed\", size = 1)\n\n\n\n\n\n\n\n\n\n\nAdd labels and title\n\nggplot(mtcars, aes (x = wt)) + \n  geom_histogram() +\n  xlab(\"Weight\") +\n  ylab (\"Count\") + \n  ggtitle(\"Weight of Cars in the MPG Dataset\")\n\n\n\n\n\n\n\n\n\n\nAdd lines to the graph\n\nggplot(mtcars, aes(x = wt)) + \n  geom_histogram () + \n  geom_vline(aes(xintercept = mean(wt))) + \n  geom_hline(aes(yintercept = 3))"
  },
  {
    "objectID": "rmanual.html#arrange-multiple-ggplot-plots",
    "href": "rmanual.html#arrange-multiple-ggplot-plots",
    "title": "R Manual",
    "section": "Arrange multiple ggplot plots",
    "text": "Arrange multiple ggplot plots\nOften it can be helpful to combine many plots of the same time (histograms, for example) into one larger plot. To do this, you will need to install the gridExtra package.\n\nlibrary(gridExtra)\n\np1 &lt;- ggplot(mtcars, aes(x = wt)) + \n  geom_histogram ()\n\np2 &lt;- ggplot(mtcars, aes(x = hp)) + \n  geom_histogram ()\n\ngrid.arrange(p1, p2)\n\n\n\n\n\n\n\n\nDetails on how to modify the graphical parameters of grid.arrange() can be found here:\ngridExtra documentation"
  },
  {
    "objectID": "rmanual.html#correlation",
    "href": "rmanual.html#correlation",
    "title": "R Manual",
    "section": "Correlation",
    "text": "Correlation\nThe default dplyr method of calculating a correlation can only do a two variable comparison.\n\nmtcars %&gt;% \n  summarize(cor(wt, mpg))\n\n  cor(wt, mpg)\n1   -0.8676594\n\n\nTo calculate the correlation between many variables, we can use the advanced package corrr as follows:\n\nlibrary(corrr)\n\nmtcars %&gt;% \n  select(c(mpg, disp, wt)) %&gt;% \n  correlate()\n\n# A tibble: 3 × 4\n  term     mpg   disp     wt\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 mpg   NA     -0.848 -0.868\n2 disp  -0.848 NA      0.888\n3 wt    -0.868  0.888 NA    \n\n\nThe corrr package has many advanced features and graphical capabilities if you need to calculate something more specific than the correlation matrix of the dataset.\nDetails on corr() can be found here:\ncorrr documentation"
  },
  {
    "objectID": "rmanual.html#simple-regression",
    "href": "rmanual.html#simple-regression",
    "title": "R Manual",
    "section": "Simple regression",
    "text": "Simple regression\n\nCalculating the regression coefficients\nThe standard way to calculate a regression is as follows, with the response variable on the left and the predictor variable on the right:\n\nfit &lt;- lm(data=mtcars, mpg ~ wt)\n\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThere is not a particularly elegant means of calculating regressions using the tidy package, however, it is often useful, particularly when conducting many regressions, to use the broom package to return the regression results in a data frame.\n\nlibrary(broom)\n\ntidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    37.3      1.88      19.9  8.24e-19\n2 wt             -5.34     0.559     -9.56 1.29e-10\n\n\nFor more details on tidy(), you can go here:\ntidy documentation\n\n\nGraphing the residuals\nTo graph the residuals, we can use the augment() function from the broom package to add in the residuals into the results.\n\naug.fit &lt;- augment(fit)\n\naug.fit\n\n# A tibble: 32 × 9\n   .rownames           mpg    wt .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 Mazda RX4          21    2.62    23.3 -2.28  0.0433   3.07 1.33e-2    -0.766 \n 2 Mazda RX4 Wag      21    2.88    21.9 -0.920 0.0352   3.09 1.72e-3    -0.307 \n 3 Datsun 710         22.8  2.32    24.9 -2.09  0.0584   3.07 1.54e-2    -0.706 \n 4 Hornet 4 Drive     21.4  3.22    20.1  1.30  0.0313   3.09 3.02e-3     0.433 \n 5 Hornet Sportabout  18.7  3.44    18.9 -0.200 0.0329   3.10 7.60e-5    -0.0668\n 6 Valiant            18.1  3.46    18.8 -0.693 0.0332   3.10 9.21e-4    -0.231 \n 7 Duster 360         14.3  3.57    18.2 -3.91  0.0354   3.01 3.13e-2    -1.31  \n 8 Merc 240D          24.4  3.19    20.2  4.16  0.0313   3.00 3.11e-2     1.39  \n 9 Merc 230           22.8  3.15    20.5  2.35  0.0314   3.07 9.96e-3     0.784 \n10 Merc 280           19.2  3.44    18.9  0.300 0.0329   3.10 1.71e-4     0.100 \n# ℹ 22 more rows\n\n\nAnd then we can graph the residuals as follows:\n\nggplot(aug.fit, aes(x=.fitted, y=.resid)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(aug.fit, aes(x=.resid)) +\n  geom_histogram()"
  },
  {
    "objectID": "rmanual.html#multiple-regression",
    "href": "rmanual.html#multiple-regression",
    "title": "R Manual",
    "section": "Multiple regression",
    "text": "Multiple regression\nMultiple regression works similarly to two variable regressions. You can easily construct advanced models (including categorical predictors and interaction terms) using the lm() command:\n\n# Three variable regression\nmod1 &lt;- lm(data=mtcars, mpg~disp+wt)\ntidy(mod1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  35.0      2.16        16.2  4.91e-16\n2 disp         -0.0177   0.00919     -1.93 6.36e- 2\n3 wt           -3.35     1.16        -2.88 7.43e- 3\n\n# Model with a categorical predictor\nmod2 &lt;- lm(data=mtcars, mpg~wt+factor(cyl))\ntidy(mod2)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     34.0      1.89      18.0  6.26e-17\n2 wt              -3.21     0.754     -4.25 2.13e- 4\n3 factor(cyl)6    -4.26     1.39      -3.07 4.72e- 3\n4 factor(cyl)8    -6.07     1.65      -3.67 9.99e- 4\n\n# Model with an interaction term\nmod3 &lt;- lm(data=mtcars, mpg~wt+factor(cyl)+factor(cyl)*wt)\ntidy(mod3)\n\n# A tibble: 6 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        39.6       3.19    12.4   2.06e-12\n2 wt                 -5.65      1.36    -4.15  3.13e- 4\n3 factor(cyl)6      -11.2       9.36    -1.19  2.44e- 1\n4 factor(cyl)8      -15.7       4.84    -3.24  3.22e- 3\n5 wt:factor(cyl)6     2.87      3.12     0.920 3.66e- 1\n6 wt:factor(cyl)8     3.45      1.63     2.12  4.34e- 2\n\n\n\nGraphing partial residual plots\nPartial residual plots help graphically display the relationship between a single predictor variable and the response variable after controlling for, or partialling out, the effect of all other predictor variables. If your regression specification meets all of the conditions for linear regression, this plot will illustrate the independent impact of the predictor variable on the response variable.\nThere are a few packages that can generate these plots, car being perhaps the best supported.\n\nlibrary(car)\n\ncrPlots(mod1, terms = ~ ., layout = NULL)\n\n\n\n\n\n\n\n\nYou can interpret these graphs as displaying the change in MPG from the mean (listed on the y axis) as displ or wt changes over its range. These are a type of marginal plots - it show the marginal impact of the predictor variable on the response variable after controlling for the impact of all other variables.\n\nNote that in the crPlot() call, the terms function is a one-sided formula that specifies a subset of the predictor variables for which you would like to generate plots. One component-plus-residual plot is drawn for each regressor. The default ~ . is to plot all numeric regressors. You can modify this term to subtract terms with the specification terms = ~ . - X3, which would plot against all regressors except for X3, while terms = ~ log(X4) would give the plot for the predictor X4 that is represented in the model by log(X4). If this argument is a quoted name of one of the predictors, the component-plus-residual plot is drawn for that predictor only.\n\n\nFor the layout option, if set to a value like c(1, 1) or c(4, 3), the layout of the graph will have this many rows and columns. If not set, the program will select an appropriate layout. If the number of graphs exceed nine, you must select the layout yourself, or you will get a maximum of nine per page.\n\nFor more details on crPlots(), you can go here:\ncrPlots documentation"
  },
  {
    "objectID": "rmanual.html#confidence-interval-for-proportion",
    "href": "rmanual.html#confidence-interval-for-proportion",
    "title": "R Manual",
    "section": "Confidence interval for proportion",
    "text": "Confidence interval for proportion\nTo calculate the confidence interval of a proportion, there are often several steps involved. First you will need to possibly recode the variable and select the cases of interest, then obtain two properties of the sample (sample proportion, sample standard deviation) and find the appropriate \\(z\\) score (\\(z^*\\)) needed for your confidence interval.\n\nRemember to check the necessary conditions first!\n\n\nRecode variable\nIt is easiet to work with the variable if successes are marked as 1 and failures maked as 0 in the dataset. You may want to recode your variable to make your calculations easier (see Recoding Variables)\n\nmtcars &lt;- mtcars %&gt;% \n  mutate(heavy = case_when(\n    tons &lt; 1.5 ~ 0,\n    tons &gt;= 1.5 ~ 1 \n  ))\n\n\n\nSample proportion and standard deviation\nFirst we need to find the key features of our sample.\n\nheavy.sum.data &lt;- mtcars %&gt;% \n  summarize(prop = mean(heavy),\n            len = length(heavy)) %&gt;% \n  mutate(sd = sqrt(prop*(1-prop)/len))\n\nheavy.sum.data\n\n   prop len         sd\n1 0.625  32 0.08558165\n\n\n\nRemember, the formula for sample sd is \\(\\sigma(\\hat{p})=\\sqrt{\\frac{pq}{n}}\\)\n\n\n\nConfidence interval\nFirst we need to calculate the margin of error (MOE). In the below example we are interested in a 95% confidence interval. qnorm(0.975) finds the appropriate value of \\(z^*\\) .\n\nheavy.sum.data &lt;- heavy.sum.data %&gt;% \n  mutate(moe = sd * qnorm(0.975))\n\nFinally, we both subtract and add the MOE to the sample proportion to find the confidence interval.\n\nheavy.sum.data$prop + c(-heavy.sum.data$moe, heavy.sum.data$moe)\n\n[1] 0.457263 0.792737"
  },
  {
    "objectID": "rmanual.html#confidence-interval-for-mean",
    "href": "rmanual.html#confidence-interval-for-mean",
    "title": "R Manual",
    "section": "Confidence interval for mean",
    "text": "Confidence interval for mean\nThe R procedure for mean confidence intervals is very similar to proportions, differing only in the method of calculating the standard deviation, which must be done using the sd() function.\n\nRemember to check the necessary conditions first!\n\n\nmpg.sum.data &lt;- mtcars %&gt;% \n  summarize(mean = mean(mpg),\n            len = length(mpg),\n            sd = sd(mpg)) %&gt;% \n  mutate(moe = sd * qnorm(0.975),\n         lower.bound = mean - moe,\n         upper.bound = mean + moe)\n\nc(mpg.sum.data$lower.bound, mpg.sum.data$upper.bound)\n\n[1]  8.278024 31.903226"
  },
  {
    "objectID": "rmanual.html#hypothesis-test-for-proportion",
    "href": "rmanual.html#hypothesis-test-for-proportion",
    "title": "R Manual",
    "section": "Hypothesis test for proportion",
    "text": "Hypothesis test for proportion\nIf we are interested in testing whether there are more heavy cars with 6 cylinders compared to the overall proportion of heavy cars, the R code for such a test is relatively straightforward.\n\nRemember to check the necessary conditions first!\n\nFirst, identify the hypotheses and calculate the sample information.\n\\(H_0: 6cyl.heavy.prop = 0.625\\)\n\\(H_a: 6cyl.heavy.prop \\neq 0.625\\)\n\nmean.heavy &lt;- mean(mtcars$heavy)\n\nhyp.test.prop &lt;- mtcars %&gt;% \n  filter(cyl==6) %&gt;% \n  summarize(prop = mean(heavy),\n            len = length(heavy),\n            sd = sqrt(mean.heavy*(1-mean.heavy)/len))\n\n\nNote that the sd is calculated as if the null hypothesis were true; we use the population mean to calculate here.\n\nNext, calculate the \\(z\\) distance between the sample and the population mean.\n\nz.score &lt;- (hyp.test.prop$prop - mean.heavy)/hyp.test.prop$sd\n\nz.score\n\n[1] -0.29277\n\n\nFinally, we find the \\(p\\) value for that difference and compare it to our \\(\\alpha\\) value.\n\n# Note that our test was specified as two sided, therefore we need to multiply by 2\np.value &lt;- (pnorm(-abs(z.score)))*2\n\np.value\n\n[1] 0.7696979\n\n\nThere is a 77% chance that one would observe a difference that large or larger from the sample mean by simple sample variation. In other words, we fail to reject the null hypothesis if our \\(\\alpha\\) is 5%."
  },
  {
    "objectID": "rmanual.html#hypothesis-test-for-mean",
    "href": "rmanual.html#hypothesis-test-for-mean",
    "title": "R Manual",
    "section": "Hypothesis test for mean",
    "text": "Hypothesis test for mean\nHypothesis test for the means are very similar, though in this case we use the sample sd to estimate the population sd.\n\nRemember to check the necessary conditions first!\n\n\nmean.mpg &lt;- mean(mtcars$mpg)\n\nhyp.test.mean &lt;- mtcars %&gt;% \n  filter(cyl==6) %&gt;% \n  summarize(mean = mean(mpg),\n            len = length(mpg),\n            sd = sd(mpg)/sqrt(len))\n\nInstead of calculating a \\(z\\) score, because of the need to correct for small sample bias, we must use the \\(t\\) distribution.\n\nt.score &lt;- (hyp.test.mean$mean - mean.mpg) / hyp.test.mean$sd\ndf.test &lt;- hyp.test.mean$len - 1\n\np.value &lt;- pt(-abs(t.score), df=df.test)*2\n\np.value\n\n[1] 0.5500829\n\n\nThere is a 55% chance that one would observe a difference that large or larger from the sample mean by simple sample variation. In other words, we fail to reject the null hypothesis if our \\(\\alpha\\) is 5%."
  },
  {
    "objectID": "rmanual.html#inference-for-difference-between-two-samples",
    "href": "rmanual.html#inference-for-difference-between-two-samples",
    "title": "R Manual",
    "section": "Inference for Difference between Two Samples",
    "text": "Inference for Difference between Two Samples\nThe R code for \\(t\\) tests is very similar to the code for hypothesis testing.\n\nfourcyl &lt;- mtcars %&gt;% \n  filter(cyl==4) %&gt;% \n  summarize(mean=mean(mpg),\n            len=length(mpg),\n            sd=sd(mpg))\n\neightcyl &lt;- mtcars %&gt;% \n  filter(cyl==8) %&gt;% \n  summarize(mean=mean(mpg),\n            len=length(mpg),\n            sd=sd(mpg))\n\ndiff.sd &lt;- sqrt(fourcyl$sd ^ 2 / fourcyl$len + eightcyl$sd ^ 2 / eightcyl$len)\ndiff.mpg.t  &lt;- (fourcyl$mean - eightcyl$mean) / diff.sd\n\n# Note that the df here is just an approximation; the textbook provides the correct formula.\n# The rule of thumb method from the the textbook recommends using the lowest df of the two samples\n# minus one.\ndiff.mpg.p  &lt;- 2 * pt(-abs(diff.mpg.t), df = min(c(fourcyl$len, eightcyl$len))-1)\n\ndiff.mpg.p\n\n[1] 1.847002e-05\n\n\nIn other words, the odds that, assuming the two samples have the same sample mean, the odds that we would see a difference between the two sample means this large or larger is effectively zero, therefore we can reject the null hypothesis.\nR does have a built-in method to do this calculation but you should understand how to achieve the same result by direct calcuation.\n\nfourcyl.data &lt;- mtcars %&gt;% \n  filter(cyl==4) %&gt;% \n  select(mpg)\n\neightcyl.data &lt;- mtcars %&gt;% \n  filter(cyl==8) %&gt;% \n  select(mpg)\n\nt.test(fourcyl.data$mpg, eightcyl.data, alternative = \"two.sided\" , mu = 0, var.equal = FALSE, conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  fourcyl.data$mpg and eightcyl.data\nt = 7.5967, df = 14.967, p-value = 1.641e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  8.318518 14.808755\nsample estimates:\nmean of x mean of y \n 26.66364  15.10000 \n\n\nDue to the first method using a rule of thumb method for calculating the degrees of freedom, the result is slightly different but very close to the R function result."
  }
]